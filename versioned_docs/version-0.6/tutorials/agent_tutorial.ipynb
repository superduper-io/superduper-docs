{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d775de07-5df4-4a80-beab-2e2dd21c9454",
   "metadata": {},
   "source": [
    "# Real time, database integrated agents, with integrated actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811333d-5007-4745-8356-00327f16217d",
   "metadata": {},
   "source": [
    "AI agents are relevant for a huge range of computer bound knowledge work, in which the individual component steps can be accessed programmatically. The range of possible agents is limited only by the tools we have to connect data, AI models in a useful environment. A very powerful-scuh environment is `superduper`.\n",
    "\n",
    "In particular, when knowledge work involves reacting to data in real time, and acting, making decisions, informing and inferring based on that data, then `superduper` is exactly the right place to start. \n",
    "\n",
    "This is show-cased in the following example:\n",
    "\n",
    "1. A `superduper` cron-job is created to search google for webpages relevant to a table of \"people-of-interest\".\n",
    "2. When new data is detected either on google (real-time) or in the table of \"people-of-interest\" (real-time), then the store of web-pages is updated.\n",
    "3. This store of web-pages is made searchable using vector-search.\n",
    "4. An additional `superduper` component is created which monitors the store of web-pages and notifies each \"person-of-interest\" when a particular topic is found in the latest google searches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6712d9-2a0a-4ed4-8db1-9efb13fa5e08",
   "metadata": {},
   "source": [
    "When considering this workflow, you will see that agents don't just need to be restricted to agent-chat, with inline actions and tool-choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23337391-cbe9-45ec-809a-3d258c6995b7",
   "metadata": {},
   "source": [
    "Create a searchapi.io key [here](https://www.searchapi.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9b0c5a-0ef3-4af2-90c3-d645b5545330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "import requests\n",
    "import typing as t\n",
    "\n",
    "\n",
    "from superduper import CronJob\n",
    "\n",
    "\n",
    "class Researcher(CronJob):\n",
    "    engine: str = 'google'\n",
    "\n",
    "    def search(self, query: str):\n",
    "        # the below logic is save on searchapi.io queries, if using the free trial\n",
    "        query = query.replace(' ', '+')\n",
    "        try:\n",
    "            with open(f'{query}.json', 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            results = requests.get(\n",
    "                \"https://www.searchapi.io/api/v1/search\",\n",
    "                headers={\"Authorization\": f\"Bearer {os.environ['SEARCHAPI_API_KEY']}\"},\n",
    "                params={\"engine\": self.engine, \"q\": query},\n",
    "            ).json()['organic_results']\n",
    "            with open(f'{query}.json', 'w') as f:\n",
    "                json.dump(results, f)\n",
    "        return results\n",
    "\n",
    "    def download_page(self, url):\n",
    "        try:\n",
    "            content = requests.get(url).text\n",
    "        except Exception as e:\n",
    "            logging.warn(str(e))\n",
    "            logging.warn('skipping')\n",
    "            return ''\n",
    "        soup = bs4.BeautifulSoup(content)\n",
    "        return soup.get_text()\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        current_poi = self.db['PersonOfInterest'].execute()\n",
    "        it = 0\n",
    "        \n",
    "        for item in current_poi:\n",
    "            search_results = self.search(f'{item[\"name\"]} {item[\"description\"]}')\n",
    "            primary_id = self.db['Page'].primary_id.execute()\n",
    "            ids = set(self.db['Page'].ids())\n",
    "            \n",
    "            for result in search_results:\n",
    "\n",
    "                      \n",
    "                new_id = self.db.databackend.create_id(result['link'])\n",
    "                \n",
    "                if new_id in ids:\n",
    "                    continue\n",
    "                \n",
    "                page = self.download_page(result['link'])\n",
    "    \n",
    "                if len(page.split('\\n')) < 10:\n",
    "                    continue\n",
    "\n",
    "                self.db['Page'].insert([{'link': result['link'], 'txt': page, primary_id: new_id, 'source': item['_id']}])\n",
    "                it += 1\n",
    "\n",
    "        logging.info(f'Found {it} new entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e664c7-68da-4721-9b33-b83c36029a35",
   "metadata": {},
   "source": [
    "To test, we'll use mongo-mock as a testing database. You can switch this to any database supported by `superduper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1a98f3-d019-4e7f-9dc1-281b51e26625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Mar-25 14:48:16.28\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.misc.importing\u001b[0m:\u001b[36m13  \u001b[0m | \u001b[1mLoading plugin: mongodb\u001b[0m\n",
      "\u001b[32m2025-Mar-25 14:48:16.39\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m51  \u001b[0m | \u001b[1mBuilding Data Layer\u001b[0m\n",
      "\u001b[32m2025-Mar-25 14:48:16.39\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m68  \u001b[0m | \u001b[1mData Layer built\u001b[0m\n",
      "\u001b[32m2025-Mar-25 14:48:16.39\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.backends.base.cluster\u001b[0m:\u001b[36m112 \u001b[0m | \u001b[1mCluster initialized in 0.00 seconds.\u001b[0m\n",
      "\u001b[32m2025-Mar-25 14:48:16.39\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.build\u001b[0m:\u001b[36m150 \u001b[0m | \u001b[1mConfiguration: \n",
      " +----------------+-------------------------------+\n",
      "| Configuration  |             Value             |\n",
      "+----------------+-------------------------------+\n",
      "|  Data Backend  |      mongomock://test_db      |\n",
      "| Artifact Store | filesystem://./artifact_store |\n",
      "|     Cache      |           in-process          |\n",
      "+----------------+-------------------------------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import superduper\n",
    "\n",
    "db = superduper('mongomock://test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b0d00-d3de-4b4c-aae4-fbcfa70c828e",
   "metadata": {},
   "source": [
    "Datapoints and components in `superduper` are dual to one another, via `Base.dict()`. In the below example, we define \n",
    "data-points which include key information about particular individuals and their contacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9a01fb-3d1f-4d03-87ad-3c940b405be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper.base import Base\n",
    "\n",
    "\n",
    "class PersonOfInterest(Base):\n",
    "    email: str\n",
    "    name: str\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd32bf-7c90-4d09-aa4d-253603c94de2",
   "metadata": {},
   "source": [
    "We will insert a single person-of-interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6e8c4f-26df-431b-8ba0-a02935d90718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Mar-25 14:51:57.68\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m408 \u001b[0m | \u001b[1mLoad (('Table', 'PersonOfInterest')) from metadata...\u001b[0m\n",
      "\u001b[32m2025-Mar-25 14:51:57.69\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m408 \u001b[0m | \u001b[1mLoad (('Table', 'PersonOfInterest')) from metadata...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['67e2b4fd6f111028bd4a1a83']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.insert([\n",
    "    PersonOfInterest(email='duncan@superduper-company.com', name='Duncan Blythe', description='AI Researcher'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff49a0-4fa0-4e48-acf7-10f01a471ef6",
   "metadata": {},
   "source": [
    "Now we will set-up the cron-job on the basis of our implementation above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8a1ec-66c9-4d35-a5b7-495ffbf2421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Listener, Table\n",
    "\n",
    "table = Table('Page', fields={'txt': 'str', 'link': 'str'})\n",
    "\n",
    "researcher = Researcher('researcher', schedule='* * * * *', upstream=[table])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f1e4e-b19c-47c8-97c0-120a84fa7cd5",
   "metadata": {},
   "source": [
    "To run this cron-job, as usual, run `db.apply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc0326-6c91-45f2-98c6-80acebc84865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db.apply(researcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6500a-2c5f-40dd-97b6-b4c75e3366ce",
   "metadata": {},
   "source": [
    "To complement the research carried out by the cron-job, we'll set up a search index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "177f8bc4-c7e8-4cde-a8f3-8b26c9a24518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import embedding\n",
    "import numpy\n",
    "\n",
    "from superduper import Model, Listener, VectorIndex\n",
    "\n",
    "\n",
    "class Embedding(Model):\n",
    "    model: str = 'text-embedding-ada-002'\n",
    "\n",
    "    def predict(self, text):\n",
    "        response = embedding(model=self.model, input=[text])\n",
    "        return numpy.array(response.data[0]['embedding'])\n",
    "\n",
    "    def predict_batches(self, texts):\n",
    "        response = embedding(model=self.model, input=texts)\n",
    "        return [\n",
    "            numpy.array(r['embedding'])\n",
    "            for r in response.data\n",
    "        ]\n",
    "\n",
    "\n",
    "myembedding = Embedding('embedding', datatype='vector[float:1265]')\n",
    "\n",
    "vector_index = VectorIndex(\n",
    "    'search_the_research',\n",
    "    indexing_listener=Listener(\n",
    "        'my_embedder',\n",
    "        model=myembedding,\n",
    "        select=db['Page'],\n",
    "        key='txt',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7606a-be60-43a4-bb2e-6f9db948f516",
   "metadata": {},
   "source": [
    "Again, we call `db.apply`. The first computations run in the main thread (initializing the computations). When additional data arrives, \n",
    "the researcher thread (created by `db.apply`) updates the vector-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180187ba-31d8-415e-bf70-2cf7da076d10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db.apply(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1bce83-2024-46d9-8806-b90beb0e9848",
   "metadata": {},
   "source": [
    "In the following cells we create another component which listens for incoming data to the `Page` table, \n",
    "and if the data fits a certain configurable description, will trigger a notification via a webhook. \n",
    "\n",
    "The example uses a Zapier webhook, which is an easy way to set up notifications and a range of actions, including payments and more.\n",
    "The choice of action is completely up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bce0556c-4955-4b1a-816d-abaa35bd7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper.components.cdc import CDC\n",
    "from superduper.components.component import trigger \n",
    "\n",
    "from litellm import completion\n",
    "import typing as t\n",
    "\n",
    "\n",
    "class Notification(CDC):\n",
    "    llm: str = 'gpt-3.5-turbo'\n",
    "    prompt: str\n",
    "    test: t.Callable\n",
    "    key: str\n",
    "\n",
    "    def _run(self, text, prompt):\n",
    "        response = completion(model=self.llm, messages=[{'content': prompt + '\\n' + text, 'role': 'user'}])\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def check(self, text):\n",
    "        response = completion(model=self.llm, messages=[{'content': self.prompt + '\\n' + text, 'role': 'user'}])\n",
    "        return self.test(self._run(text, self.prompt))\n",
    "\n",
    "    def summarize(self, text):\n",
    "        return self._run(text, 'Summarize this text in 20 words or less; add no padding or preamble, just the summary')\n",
    "\n",
    "    def send_notification(self, summary, email):\n",
    "        logging.info(f'sending message: {summary} to {email}')\n",
    "        return\n",
    "        response = requests.post(\n",
    "            'https://hooks.zapier.com/hooks/catch/225673205/1234oq/',\n",
    "            data={'text': summary, 'email': email},\n",
    "        )\n",
    "\n",
    "    @trigger('apply', 'insert')\n",
    "    def notify(self, ids: t.List[str] | None = None):\n",
    "        if not ids:\n",
    "            data = self.db[self.cdc_table].execute()\n",
    "        else:\n",
    "            data = self.db[self.cdc_table].subset(ids)\n",
    "\n",
    "        messages = []\n",
    "        for r in data:\n",
    "            if self.check(r[self.key]):\n",
    "                summary = self.summarize(r[self.key])\n",
    "                email = self.db['PersonOfInterest'].get(primary_id=r['source'])['email']\n",
    "                messages.append({'summary': summary, 'email': email})\n",
    "\n",
    "        for message in messages:\n",
    "            self.send_notification(**message)\n",
    "        return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce7b3781-1d44-4bf9-8531-4e98bff11585",
   "metadata": {},
   "outputs": [],
   "source": [
    "notification = Notification(\n",
    "    'notification', \n",
    "    prompt='Tell me if this text concerns software engineering in some way (yes/no - lowercase); here is the text:\\n',\n",
    "    cdc_table='Page',\n",
    "    test=lambda x: x.lower().strip() == 'yes',\n",
    "    key='txt',\n",
    "    db=db,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a852d3a-918f-459a-8d71-0040753968c6",
   "metadata": {},
   "source": [
    "As before, since this notification is a change-data-capture component and has `@trigger(..., 'insert')`, \n",
    "it will respond to incoming data in the researcher thread, as well as checking the existing data on first `db.apply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727da3a-f38f-46b7-8f74-cf32b41fdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.apply(notification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b0a26-c0bd-4691-aa0d-fe5217d79066",
   "metadata": {},
   "source": [
    "Are you happy with your setup? The whole setup can now be saved/ bundled as an `Application`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8431cd6-f03e-4d81-beb9-5d5c5b79330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Application\n",
    "\n",
    "application = Application(\n",
    "    'online_trend_notification',\n",
    "    components=[notification, vector_index, listener],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343a025-8f64-4265-af50-c2029f5ddfe3",
   "metadata": {},
   "source": [
    "The next time you do this, you can apply everything in one go, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabbb08a-ce62-46e2-9957-e766db38b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.apply(application)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
