"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[410],{2869:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var a=t(4848),r=t(8453);const i={},s="PDF RAG",l={id:"templates/pdf_rag",title:"PDF RAG",description:"This is a PDF-based RAG application. While answering questions, it accesses relevant information from the PDF and displays the corresponding paragraphs in the form of images.",source:"@site/docs/templates/pdf_rag.md",sourceDirName:"templates",slug:"/templates/pdf_rag",permalink:"/docs/next/templates/pdf_rag",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/docs/templates/pdf_rag.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Multimodal vector search - Video",permalink:"/docs/next/templates/multimodal_video_search"},next:{title:"Simple retrieval augmented generation with OpenAI",permalink:"/docs/next/templates/simple_rag"}},o={},d=[{value:"Create a table to store PDFs.",id:"create-a-table-to-store-pdfs",level:2},{value:"Split the PDF file into images for later result display",id:"split-the-pdf-file-into-images-for-later-result-display",level:2},{value:"Build a chunks model and return chunk results with coordinate information.",id:"build-a-chunks-model-and-return-chunk-results-with-coordinate-information",level:2},{value:"Build a vector index for vector search",id:"build-a-vector-index-for-vector-search",level:2},{value:"Create a plugin",id:"create-a-plugin",level:2},{value:"Create a RAG model",id:"create-a-rag-model",level:2},{value:"Create template",id:"create-template",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"pdf-rag",children:"PDF RAG"}),"\n",(0,a.jsx)(n.p,{children:"This is a PDF-based RAG application. While answering questions, it accesses relevant information from the PDF and displays the corresponding paragraphs in the form of images."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"APPLY = True\nEAGER = False\nCOLLECTION_NAME = '<var:table_name>' if not APPLY else 'sample_pdf_rag'\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import superduper, CFG\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"db = superduper('mongomock://test')\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def getter():\n    import subprocess\n    import os\n\n    subprocess.run(['curl', '-O', 'https://superduperdb-public-demo.s3.amazonaws.com/pdfs.zip'])\n    subprocess.run(['unzip', '-o', 'pdfs.zip'])\n    subprocess.run(['rm', 'pdfs.zip'])\n    pdf_folder = \"pdfs\"\n    pdf_names = [pdf for pdf in os.listdir(pdf_folder) if pdf.endswith(\".pdf\")]\n    pdf_paths = [os.path.join(pdf_folder, pdf) for pdf in pdf_names]\n    data = [{\"url\": pdf_path, \"file\": pdf_path} for pdf_path in pdf_paths]\n    return data\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    data = getter()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"create-a-table-to-store-pdfs",children:"Create a table to store PDFs."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\nfrom superduper import Table\n\ntable = Table(identifier=COLLECTION_NAME, fields={'url': 'str', 'file': 'file'})\n\nif APPLY:\n    db.apply(table, force=True)\n    db[COLLECTION_NAME].insert(data)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"db.show()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"split-the-pdf-file-into-images-for-later-result-display",children:"Split the PDF file into images for later result display"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"!pip install pdf2image\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from superduper import ObjectModel, Listener, logging\n\nfrom pdf2image import convert_from_path\nimport os\n\n\ndef split_image(pdf_path):\n    if hasattr(pdf_path, \'unpack\'):\n        pdf_path = pdf_path.unpack()\n    \n    logging.info(f"Splitting images from {pdf_path}")\n\n    image_folders = "data/pdf-images"\n    pdf_name = os.path.basename(pdf_path)\n    images = convert_from_path(pdf_path)\n    logging.info(f"Number of images: {len(images)}")\n\n    image_folder = os.path.join(image_folders, pdf_name)\n    if not os.path.exists(image_folder):\n        os.makedirs(image_folder)\n\n    data = []\n    for i, image in enumerate(images):\n        path = os.path.join(image_folder, f"{i}.jpg")\n        image.save(os.path.join(path))\n        data.append(path)\n    return data\n\n\nmodel_split_image = ObjectModel(\n    identifier="split_image",\n    object=split_image,\n    datatype=\'file\',\n)\n\nlistener_split_image = Listener(\n    \'split_image\', \n    model=model_split_image,\n    key="file",\n    select=db[COLLECTION_NAME],\n    flatten=True,\n)\n\nif EAGER and APPLY:\n    db.apply(listener_split_image, force=True)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"build-a-chunks-model-and-return-chunk-results-with-coordinate-information",children:"Build a chunks model and return chunk results with coordinate information."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def remove_sidebars(elements):\n    import re\n    from collections import defaultdict\n\n    from unstructured.documents.elements import ElementType\n\n    if not elements:\n        return elements\n    points_groups = defaultdict(list)\n    min_x = 99999999\n    max_x = 0\n    e2index = {e.id: i for i, e in enumerate(elements)}\n    for e in elements:\n        x_l = int(e.metadata.coordinates.points[0][0])\n        x_r = int(e.metadata.coordinates.points[2][0])\n        points_groups[(x_l, x_r)].append(e)\n        min_x = min(min_x, x_l)\n        max_x = max(max_x, x_r)\n    sidebars_elements = set()\n    for (x_l, x_r), es in points_groups.items():\n        first_id = e2index[es[0].id]\n        last_id = e2index[es[-1].id]\n        on_left = first_id == 0 and x_l == min_x\n        on_right = (last_id == len(elements) - 2) and x_r == max_x\n        loc_match = [on_left, on_right]\n        total_text = "".join(map(str, es))\n        condiction = [\n            any(loc_match),\n            len(es) >= 3,\n            re.findall("^[A-Z\\s\\d,]+$", total_text),\n        ]\n        if not all(condiction):\n            continue\n        sidebars_elements.update(map(lambda x: x.id, es))\n        if on_left:\n            check_page_num_e = elements[last_id + 1]\n        else:\n            check_page_num_e = elements[-1]\n        if (\n            check_page_num_e.category == ElementType.UNCATEGORIZED_TEXT\n            and check_page_num_e.text.strip().isalnum()\n        ):\n            sidebars_elements.add(check_page_num_e.id)\n\n    elements = [e for e in elements if e.id not in sidebars_elements]\n    return elements\n\n\ndef remove_annotation(elements):\n    from collections import Counter\n\n    from unstructured.documents.elements import ElementType\n\n    page_num = max(e.metadata.page_number for e in elements)\n    un_texts_counter = Counter(\n        [e.text for e in elements if e.category == ElementType.UNCATEGORIZED_TEXT]\n    )\n    rm_text = set()\n    for text, count in un_texts_counter.items():\n        if count / page_num >= 0.5:\n            rm_text.add(text)\n    elements = [e for e in elements if e.text not in rm_text]\n    return elements\n\n\ndef create_chunk_and_metadatas(page_elements, stride=3, window=10):\n    page_elements = remove_sidebars(page_elements)\n    for index, page_element in enumerate(page_elements):\n        page_element.metadata.num = index\n    datas = []\n    for i in range(0, len(page_elements), stride):\n        windown_elements = page_elements[i : i + window]\n        chunk = "\\n".join([e.text for e in windown_elements])\n        source_elements = [e.to_dict() for e in windown_elements]\n        datas.append(\n            {\n                "txt": chunk,\n                "source_elements": source_elements,\n            }\n        )\n    return datas\n\n\ndef get_chunks(pdf):\n    from collections import defaultdict\n    from unstructured.documents.coordinates import RelativeCoordinateSystem\n    from unstructured.partition.pdf import partition_pdf\n\n    if hasattr(pdf, \'unpack\'):\n        pdf = pdf.unpack()\n\n    elements = partition_pdf(pdf)\n    elements = remove_annotation(elements)\n\n    pages_elements = defaultdict(list)\n    for element in elements:\n        element.convert_coordinates_to_new_system(\n            RelativeCoordinateSystem(), in_place=True\n        )\n        pages_elements[element.metadata.page_number].append(element)\n\n    all_chunks_and_links = sum(\n        [\n            create_chunk_and_metadatas(page_elements)\n            for _, page_elements in pages_elements.items()\n        ],\n        [],\n    )\n    return all_chunks_and_links\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"model_chunk = ObjectModel(\n    identifier=\"chunk\",\n    object=get_chunks,\n    datatype='json',\n)\n\nlistener_chunk = Listener(\n    'chunker',\n    key='file',\n    model=model_chunk,\n    select=db[COLLECTION_NAME],\n    flatten=True,\n)\n\nif EAGER and APPLY:\n    db.apply(listener_chunk, force=True)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"build-a-vector-index-for-vector-search",children:"Build a vector index for vector search"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper_openai import OpenAIEmbedding\n\nopenai_embedding = OpenAIEmbedding(identifier='embedding', model='text-embedding-ada-002', datatype='vector[float:1536]')\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from superduper_openai.model import OpenAIEmbedding\nfrom superduper import VectorIndex\n\nlistener_embedding = Listener(\n    \'embedding\',\n    model=openai_embedding,\n    key=f"{listener_chunk.outputs}.txt",\n    select=db[listener_chunk.outputs].select(),\n)\n\nvector_index = VectorIndex(\n    identifier="vector-index",\n    indexing_listener=listener_embedding,\n)\n\nif EAGER and APPLY:\n    db.apply(vector_index, force=True)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"create-a-plugin",children:"Create a plugin"}),"\n",(0,a.jsx)(n.p,{children:"When applying the processor, saves the plugin in the database, thereby saving the related dependencies as well."}),"\n",(0,a.jsx)(n.p,{children:"The processor will integrate the returned chunks information with the images, and return a visualized image.\u200b"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from superduper import Plugin\nfrom utils import Processor\n\n\nprocessor = Processor(\n    identifier="processor",\n    chunk_key=listener_chunk.outputs,\n    split_image_key=listener_split_image.outputs,\n    upstream=[Plugin(path="./utils.py")],\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"create-a-rag-model",children:"Create a RAG model"}),"\n",(0,a.jsx)(n.p,{children:"Create a RAG model to perform retrieval-augmented generation (RAG) and return the results."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper_openai import OpenAIChatCompletion\n\nllm_openai = OpenAIChatCompletion(identifier='llm-openai', model='gpt-3.5-turbo')\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from superduper_openai.model import OpenAIChatCompletion\nfrom utils import Rag\n\nprompt_template = (\n    "The following is a document and question\\n"\n    "Only provide a very concise answer\\n"\n    "Context:\\n\\n"\n    "{context}\\n\\n"\n    "Here\'s the question:{query}\\n"\n    "answer:"\n)\n\nrag = Rag(\n    identifier="rag",\n    llm_model=llm_openai,\n    vector_index=vector_index, \n    prompt_template=prompt_template,\n    processor=processor,\n    upstream=[vector_index],\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from utils import Rag\n\nRag.__module__\n"})}),"\n",(0,a.jsx)(n.h2,{id:"create-template",children:"Create template"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import Application\n\napp = Application(\n    'pdf-rag',\n    components=[\n        table,\n        listener_split_image,\n        listener_chunk,\n        vector_index,\n        rag\n    ]\n    \n)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    db.apply(app, force=True)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from IPython.display import Image, Markdown, display\n\nif APPLY:\n    db.apply(rag, force=True)\n    result = rag.predict("Tell me about GPT on the basis of these data.", format_result=True)\n    \n    display(Markdown(result["answer"]))\n    \n    for message, img in result["images"]:\n        display(Markdown(message))\n        display(img)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import Template, CFG, Table\nfrom superduper.components.dataset import RemoteData\n\ntemplate = Template(\n    'pdf-rag',\n    db=db,\n    template=app,\n    substitutions={\n        prompt_template: \n        'prompt_template',\n        COLLECTION_NAME: 'table_name',\n        'gpt-3.5-turbo': 'llm_model',\n        'text-embedding-ada-002': 'embedding_model'\n    },\n    template_variables=['table_name', 'prompt_template', 'llm_model', 'embedding_model'],\n    default_tables=[Table(\n        'sample_pdf_rag',\n        fields={\"url\": \"str\", \"file\": 'file'},\n        data=RemoteData('sample_pdfs', getter=getter),\n    )],\n    types={\n        'prompt_template':{\n            'type': 'str',\n            'default': prompt_template\n        },\n        'table_name': {\n            'type': 'str',\n            'default': 'sample_pdf_rag'\n        },\n        'llm_model': {\n            'type': 'str',\n            'default': 'gpt-3.5-turbo',\n        },\n        'embedding_model': {\n            'type': 'str',\n            'default': 'text-embedding-ada-002',\n        },\n    }\n)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'template.export(".")\n'})})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var a=t(6540);const r={},i=a.createContext(r);function s(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);