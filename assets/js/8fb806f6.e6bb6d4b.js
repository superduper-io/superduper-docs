"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[6421],{9335:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var r=t(4848),a=t(8453);const i={},s="Simple retrieval augmented generation with OpenAI",l={id:"templates/simple_rag",title:"Simple retrieval augmented generation with OpenAI",description:"Connect to superduper",source:"@site/docs/templates/simple_rag.md",sourceDirName:"templates",slug:"/templates/simple_rag",permalink:"/docs/next/templates/simple_rag",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/docs/templates/simple_rag.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Retrieval augmented generation",permalink:"/docs/next/templates/rag"},next:{title:"Text vector search",permalink:"/docs/next/templates/text_vector_search"}},o={},d=[{value:"Connect to superduper",id:"connect-to-superduper",level:2},{value:"Insert simple data",id:"insert-simple-data",level:2},{value:"Apply a chunker for search",id:"apply-a-chunker-for-search",level:2},{value:"Select outputs of upstream listener",id:"select-outputs-of-upstream-listener",level:2},{value:"Build text embedding model",id:"build-text-embedding-model",level:2},{value:"Create vector-index",id:"create-vector-index",level:2},{value:"Build LLM",id:"build-llm",level:2},{value:"Answer question with LLM",id:"answer-question-with-llm",level:2},{value:"Create template",id:"create-template",level:2}];function p(e){const n={admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"simple-retrieval-augmented-generation-with-openai",children:"Simple retrieval augmented generation with OpenAI"}),"\n",(0,r.jsx)(n.h2,{id:"connect-to-superduper",children:"Connect to superduper"}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:'Note that this is only relevant if you are running superduper in development mode.\nOtherwise refer to "Configuring your production system".'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"APPLY = True\nSAMPLE_COLLECTION_NAME = 'sample_simple_rag'\nCOLLECTION_NAME = '<var:table_name>' if not APPLY else 'docs'\nID_FIELD = '<var:id_field>' if not APPLY else 'id'\nOUTPUT_PREFIX = 'outputs__'\nEAGER = False\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import superduper, CFG\nimport os\n\ndb = superduper('mongomock://', initialize_cluster=False)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import json\nimport requests\nimport io\nfrom superduper import logging\n\n\ndef getter():\n    logging.info('Downloading data...')\n    response = requests.get('https://superduperdb-public-demo.s3.amazonaws.com/text.json')\n    logging.info('Downloading data... (Done)')\n    data = json.loads(response.content.decode('utf-8'))\n    return [{'x': r} for r in data]\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    data = getter()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"insert-simple-data",children:"Insert simple data"}),"\n",(0,r.jsx)(n.p,{children:"After turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    from superduper import Document, Table\n    table = Table(COLLECTION_NAME, fields={'x': 'str'})\n    db.apply(table, force=True)\n    ids = db[COLLECTION_NAME].insert(data)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Create plugin:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Plugin\n\nplugin = Plugin(path='./rag_plugin.py')\n"})}),"\n",(0,r.jsx)(n.h2,{id:"apply-a-chunker-for-search",children:"Apply a chunker for search"}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["Note that applying a chunker is ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"not"})})," mandatory for search.\nIf your data is already chunked (e.g. short text snippets or audio) or if you\nare searching through something like images, which can't be chunked, then this\nwon't be necessary."]})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Listener\nfrom rag_plugin import Chunker\n\nupstream_listener = Listener(\n    model=Chunker(identifier='chunker'),\n    select=db[COLLECTION_NAME],\n    key='x',\n    identifier='chunker',\n    flatten=True,\n    upstream=[plugin],\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY and EAGER:\n    db.apply(upstream_listener, force=True)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"select-outputs-of-upstream-listener",children:"Select outputs of upstream listener"}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:"This is useful if you have performed a first step, such as pre-computing\nfeatures, or chunking your data. You can use this query to\noperate on those outputs."})}),"\n",(0,r.jsx)(n.h2,{id:"build-text-embedding-model",children:"Build text embedding model"}),"\n",(0,r.jsx)(n.p,{children:"OpenAI:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\n\nfrom superduper_openai import OpenAIEmbedding\n\nopenai_embedding = OpenAIEmbedding(\n    identifier='text-embedding',\n    model='text-embedding-ada-002',\n    datatype='vector[float:1536]',\n)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"create-vector-index",children:"Create vector-index"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import VectorIndex, Listener\n\nvector_index_name = 'vectorindex'\n\nvector_index = VectorIndex(\n    vector_index_name,\n    indexing_listener=Listener(\n        key=upstream_listener.outputs,\n        select=db[upstream_listener.outputs],\n        model=openai_embedding,\n        identifier='embeddinglistener',\n        upstream=[upstream_listener],\n    )\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY and EAGER:\n    db.apply(vector_index, force=True)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"build-llm",children:"Build LLM"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper_openai import OpenAIChatCompletion\n\n\nllm_openai = OpenAIChatCompletion(\n    identifier='llm-model',\n    model='gpt-3.5-turbo',\n    datatype='str',\n)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"answer-question-with-llm",children:"Answer question with LLM"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rag_plugin import RAGModel\n\n\nprompt_template = (\n    "Use the following context snippets, these snippets are not ordered!, Answer the question based on this context.\\n"\n    "These snippets are samples from our internal data-repositories, and should be used exclusively and as a matter"\n    " of priority to answer the question. Please answer in 20 words or less.\\n\\n"\n    "{context}\\n\\n"\n    "Here\'s the question: {query}"\n)\n\nrag = RAGModel(\n    \'simple_rag\',\n    select=db[upstream_listener.outputs].select().like({upstream_listener.outputs: \'<var:query>\'}, vector_index=vector_index_name, n=5),\n    prompt_template=prompt_template,\n    key=upstream_listener.outputs,\n    llm=llm_openai,\n)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY and EAGER:\n    db.apply(rag, force=True)\n"})}),"\n",(0,r.jsx)(n.p,{children:"By applying the RAG model to the database, it will subsequently be accessible for use in other services."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Streamlit, Plugin\nfrom rag_plugin import demo_func\n\ndemo = Streamlit('simple-rag-demo', demo_func=demo_func)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Application\n\napp = Application(\n    'simple-rag-app',\n    upstream=[plugin],\n    components=[\n        upstream_listener,\n        vector_index,\n        rag,\n        demo,\n    ]\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    db.apply(app, force=True)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    rag = db.load('RAGModel', 'simple_rag')\n    print(rag.predict('Tell me about vector-search in the project and the use of lance.'))\n"})}),"\n",(0,r.jsx)(n.p,{children:"You can now load the model elsewhere and make predictions using the following command."}),"\n",(0,r.jsx)(n.h2,{id:"create-template",children:"Create template"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Template, Table, Schema\nfrom superduper.components.dataset import RemoteData\n\ntemplate = Template(\n    'simple_rag',\n    template=app,\n    substitutions={\n        COLLECTION_NAME: 'table_name',\n        'text-embedding-ada-002': 'embedding_model',\n        'gpt-3.5-turbo': 'llm_model',\n    },\n    template_variables=['table_name', 'id_field', 'embedding_model', 'llm_model'],\n    default_tables=[\n        Table(\n            'sample_simple_rag',\n            fields={'x': 'str'},\n            data=RemoteData(\n                'superduper-docs',\n                getter=getter,\n            )\n        ),\n    ],\n    types={\n        'id_field': {\n            'type': 'str',\n            'default': '_id',\n        },\n        'embedding_model': {\n            'type': 'str',\n            'default': 'text-embedding-ada-002',\n            'choices': ['text-embedding-ada-002', 'nomic-embed-text:latest'],\n        },\n        'llm_model': {\n            'type': 'str',\n            'default': 'gpt-3.5-turbo',\n            'choices': ['gpt-3.5-turbo', 'gpt-4-turbo', 'llama3.1:8b']\n        },\n        'table_name': {\n            'type': 'str',\n            'default': SAMPLE_COLLECTION_NAME,\n        }\n    },\n    schema={\n        \"id_field\": \"id_field\",\n        \"embedding_model\": \"embedding_model\",\n        \"llm_model\": \"llm_model\",\n        \"table_name\": \"table_name\",\n    },\n    db=db\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"template.export('.')\n"})})]})}function c(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var r=t(6540);const a={},i=r.createContext(a);function s(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);