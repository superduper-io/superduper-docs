"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[6421],{9335:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>i,metadata:()=>o,toc:()=>d});var r=t(4848),s=t(8453);const i={},a="Simple retrieval augmented generation with OpenAI",o={id:"templates/simple_rag",title:"Simple retrieval augmented generation with OpenAI",description:"Connect to superduper",source:"@site/docs/templates/simple_rag.md",sourceDirName:"templates",slug:"/templates/simple_rag",permalink:"/docs/templates/simple_rag",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/docs/templates/simple_rag.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Retrieval augmented generation",permalink:"/docs/templates/rag"},next:{title:"Text vector search",permalink:"/docs/templates/text_vector_search"}},l={},d=[{value:"Connect to superduper",id:"connect-to-superduper",level:2},{value:"Insert simple data",id:"insert-simple-data",level:2},{value:"Apply a chunker for search",id:"apply-a-chunker-for-search",level:2},{value:"Select outputs of upstream listener",id:"select-outputs-of-upstream-listener",level:2},{value:"Build text embedding model",id:"build-text-embedding-model",level:2},{value:"Create vector-index",id:"create-vector-index",level:2},{value:"Build LLM",id:"build-llm",level:2},{value:"Answer question with LLM",id:"answer-question-with-llm",level:2},{value:"Create template",id:"create-template",level:2}];function p(e){const n={admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"simple-retrieval-augmented-generation-with-openai",children:"Simple retrieval augmented generation with OpenAI"}),"\n",(0,r.jsx)(n.h2,{id:"connect-to-superduper",children:"Connect to superduper"}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:'Note that this is only relevant if you are running superduper in development mode.\nOtherwise refer to "Configuring your production system".'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"APPLY = True\nCOLLECTION_NAME = '<var:table_name>' if not APPLY else '_sample_rag'\nID_FIELD = '<var:id_field>' if not APPLY else 'id'\nOUTPUT_PREFIX = 'outputs__'\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import superduper, CFG\n\nCFG.output_prefix = OUTPUT_PREFIX\nCFG.bytes_encoding = 'str'\nCFG.native_json = False\n\ndb = superduper()\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"db.drop(force=True, data=True)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import json\n\nwith open('data.json', 'r') as f:\n    data = json.load(f)\ndata = [{'x': r} for r in data]\n"})}),"\n",(0,r.jsx)(n.h2,{id:"insert-simple-data",children:"Insert simple data"}),"\n",(0,r.jsx)(n.p,{children:"After turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    from superduper import Document\n    ids = db.execute(db[COLLECTION_NAME].insert([Document(r) for r in data]))\n"})}),"\n",(0,r.jsx)(n.h2,{id:"apply-a-chunker-for-search",children:"Apply a chunker for search"}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["Note that applying a chunker is ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"not"})})," mandatory for search.\nIf your data is already chunked (e.g. short text snippets or audio) or if you\nare searching through something like images, which can't be chunked, then this\nwon't be necessary."]})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Model\n\n\nclass Chunker(Model):\n    chunk_size: int = 200\n    signature: str = 'singleton'\n\n    def predict(self, text):\n        text = text.split()\n        chunks = [' '.join(text[i:i + self.chunk_size]) for i in range(0, len(text), self.chunk_size)]\n        return chunks\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Now we apply this chunker to the data by wrapping the chunker in ",(0,r.jsx)(n.code,{children:"Listener"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Listener\n\n\nupstream_listener = Listener(\n    model=Chunker(identifier='chunker'),\n    select=db[COLLECTION_NAME].select(ID_FIELD, 'x'),\n    key='x',\n    identifier='chunker',\n    flatten=True,\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    db.apply(upstream_listener, force=True)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"select-outputs-of-upstream-listener",children:"Select outputs of upstream listener"}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:"This is useful if you have performed a first step, such as pre-computing\nfeatures, or chunking your data. You can use this query to\noperate on those outputs."})}),"\n",(0,r.jsx)(n.h2,{id:"build-text-embedding-model",children:"Build text embedding model"}),"\n",(0,r.jsx)(n.p,{children:"OpenAI:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nfrom superduper.components.vector_index import sqlvector\n\nfrom superduper_openai import OpenAIEmbedding\n\nopenai_embedding = OpenAIEmbedding(identifier='text-embedding-ada-002' , datatype=sqlvector(shape=(1536,)))\n"})}),"\n",(0,r.jsx)(n.h2,{id:"create-vector-index",children:"Create vector-index"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import VectorIndex, Listener\n\nvector_index_name = 'vectorindex'\n\nvector_index = VectorIndex(\n    vector_index_name,\n    indexing_listener=Listener(\n        key=upstream_listener.outputs,\n        select=db[upstream_listener.outputs].select('id', '_source', upstream_listener.outputs),\n        model=openai_embedding,\n        identifier='embeddinglistener',\n        upstream=[upstream_listener],\n    )\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    db.apply(vector_index, force=True)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"build-llm",children:"Build LLM"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper_openai import OpenAIChatCompletion\n\nllm_openai = OpenAIChatCompletion(identifier='llm-openai', model='gpt-3.5-turbo')\n"})}),"\n",(0,r.jsx)(n.h2,{id:"answer-question-with-llm",children:"Answer question with LLM"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from superduper import model\nfrom superduper.components.model import RAGModel\n\nprompt_template = (\n    "Use the following context snippets, these snippets are not ordered!, Answer the question based on this context.\\n"\n    "{context}\\n\\n"\n    "Here\'s the question: {query}"\n)\n\nrag = RAGModel(\n    \'rag-model\',\n    select=db[upstream_listener.outputs].select().like({upstream_listener.outputs: \'<var:query>\'}, vector_index=vector_index_name, n=5),\n    prompt_template=prompt_template,\n    key=upstream_listener.outputs,\n    llm=llm_openai,\n)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    db.apply(rag, force=True)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    print(rag.predict('Tell me about vector-search'))\n"})}),"\n",(0,r.jsx)(n.p,{children:"By applying the RAG model to the database, it will subsequently be accessible for use in other services."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Application\n\napp = Application(\n    'rag-app',\n    components=[\n        upstream_listener,\n        vector_index,\n        rag,\n    ]\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    db.apply(app, force=True)\n"})}),"\n",(0,r.jsx)(n.p,{children:"You can now load the model elsewhere and make predictions using the following command."}),"\n",(0,r.jsx)(n.h2,{id:"create-template",children:"Create template"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Template\n\ntemplate = Template(\n    'rag-simple',\n    template=app,\n    data=data,\n    substitutions={'docs': 'table_name', OUTPUT_PREFIX: 'output_prefix'},\n    template_variables=['table_name', 'id_field'],\n    types={\n        'id_field': {\n            'type': 'str',\n            'default': '_id',\n        },\n        'llm_model': {\n            'type': 'str',\n            'choices': ['openai', 'anthropic', 'vllm', 'llamacpp'],\n            'default': 'openai',\n        },\n        'embedding_model': {\n            'type': 'str',\n            'choices': ['openai', 'sentence_transformers'],\n            'default': 'openai',\n        },\n        'table_name': {\n            'type': 'str',\n            'default': '_sample_rag'\n        },\n        'output_prefix': {\n            'type': 'str',\n            'default': OUTPUT_PREFIX,\n        }\n    }\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"template.export('.')\n"})})]})}function c(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const s={},i=r.createContext(s);function a(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);