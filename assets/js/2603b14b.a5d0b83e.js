"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[1041],{9971:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>d,toc:()=>l});var i=n(4848),o=n(8453);const a={},r="Multimodal vector search - images",d={id:"templates/multimodal_image_search",title:"Multimodal vector search - images",description:"Connect to superduper",source:"@site/docs/templates/multimodal_image_search.md",sourceDirName:"templates",slug:"/templates/multimodal_image_search",permalink:"/docs/templates/multimodal_image_search",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/docs/templates/multimodal_image_search.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Fine tune LLM on database",permalink:"/docs/templates/llm_finetuning"},next:{title:"Multimodal vector search - Video",permalink:"/docs/templates/multimodal_video_search"}},s={},l=[{value:"Connect to superduper",id:"connect-to-superduper",level:2},{value:"Get useful sample data",id:"get-useful-sample-data",level:2},{value:"Build multimodal embedding models",id:"build-multimodal-embedding-models",level:2},{value:"Create vector-index",id:"create-vector-index",level:2},{value:"Add the data",id:"add-the-data",level:2},{value:"Perform a vector search",id:"perform-a-vector-search",level:2},{value:"Visualize Results",id:"visualize-results",level:2},{value:"Create a <code>Template</code>",id:"create-a-template",level:2}];function c(e){const t={admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"multimodal-vector-search---images",children:"Multimodal vector search - images"}),"\n",(0,i.jsx)(t.h2,{id:"connect-to-superduper",children:"Connect to superduper"}),"\n",(0,i.jsx)(t.admonition,{type:"note",children:(0,i.jsx)(t.p,{children:'Note that this is only relevant if you are running superduper in development mode.\nOtherwise refer to "Configuring your production system".'})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"from superduper import superduper\n\ndb = superduper('mongomock:///test_db')\n"})}),"\n",(0,i.jsx)(t.h2,{id:"get-useful-sample-data",children:"Get useful sample data"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"!curl -O https://superduperdb-public-demo.s3.amazonaws.com/images.zip && unzip images.zip\nimport os\nfrom PIL import Image\n\ndata = [f'images/{x}' for x in os.listdir('./images') if x.endswith(\".png\")][:200]\ndata = [ Image.open(path) for path in data]\n"})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"data = [{'img': d} for d in data[:100]]\n"})}),"\n",(0,i.jsx)(t.h2,{id:"build-multimodal-embedding-models",children:"Build multimodal embedding models"}),"\n",(0,i.jsx)(t.p,{children:"We define the output data type of a model as a vector for vector transformation."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"# <tab: MongoDB>\nfrom superduper.components.vector_index import vector\noutput_datatpye = vector(shape=(1024,))\n"})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"# <tab: SQL>\nfrom superduper.components.vector_index import sqlvector\noutput_datatpye = sqlvector(shape=(1024,))\n"})}),"\n",(0,i.jsx)(t.p,{children:"Then define two models, one for text embedding and one for image embedding."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"!pip install git+https://github.com/openai/CLIP.git\n!pip install ../../plugins/torch\nimport clip\nfrom superduper import vector\nfrom superduper_torch import TorchModel\n\n# Load the CLIP model and obtain the preprocessing function\nmodel, preprocess = clip.load(\"RN50\", device='cpu')\n\n# Create a TorchModel for text encoding\ncompatible_model = TorchModel(\n    identifier='clip_text', # Unique identifier for the model\n    object=model, # CLIP model\n    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n    datatype=output_datatpye,  # Vector encoder with shape (1024,)\n    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n)\n\n# Create a TorchModel for visual encoding\nembedding_model = TorchModel(\n    identifier='clip_image',  # Unique identifier for the model\n    object=model.visual,  # Visual part of the CLIP model    \n    preprocess=preprocess, # Visual preprocessing using CLIP\n    postprocess=lambda x: x.tolist(), # Convert the output to a list \n    datatype=output_datatpye, # Vector encoder with shape (1024,)\n)\n"})}),"\n",(0,i.jsx)(t.p,{children:"Because we use multimodal models, we define different keys to specify which model to use for embedding calculations in the vector_index."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"indexing_key = 'img' # we use img key for img embedding\ncompatible_key = 'text' # we use text key for text embedding\n"})}),"\n",(0,i.jsx)(t.h2,{id:"create-vector-index",children:"Create vector-index"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"vector_index_name = 'my-vector-index'\n"})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"from superduper import VectorIndex, Listener\n\nvector_index = VectorIndex(\n    vector_index_name,\n    indexing_listener=Listener(\n        key=indexing_key,                 # the `Document` key `model` should ingest to create embedding\n        select=db['docs'].select(),       # a `Select` query telling which data to search over\n        model=embedding_model,            # a `_Predictor` how to convert data to embeddings\n        identifier='indexing-listener',\n    ),\n    compatible_listener=Listener(\n        key=compatible_key,               # the `Document` key `model` should ingest to create embedding\n        model=compatible_model,           # a `_Predictor` how to convert data to embeddings\n        select=None,\n        identifier='compatible-listener',\n    )\n)\n"})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"from superduper import Application\n\napplication = Application(\n    'image-vector-search',\n    components=[vector_index],\n)\n\ndb.apply(application)\n"})}),"\n",(0,i.jsx)(t.h2,{id:"add-the-data",children:"Add the data"}),"\n",(0,i.jsxs)(t.p,{children:["The order in which data is added is not important. ",(0,i.jsx)(t.em,{children:"However"})," if your data requires a custom ",(0,i.jsx)(t.code,{children:"Schema"})," in order to work, it's easier to add the ",(0,i.jsx)(t.code,{children:"Application"})," first, and the data later. The advantage of this flexibility, is that once the ",(0,i.jsx)(t.code,{children:"Application"})," is installed, it's waiting for incoming data, so that the ",(0,i.jsx)(t.code,{children:"Application"})," is always up-to-date. This comes in particular handy with AI scenarios which need to respond to changing news."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"from superduper import Document\n\ntable_or_collection = db['docs']\n\nids = db.execute(table_or_collection.insert([Document(r) for r in data]))\n"})}),"\n",(0,i.jsx)(t.h2,{id:"perform-a-vector-search",children:"Perform a vector search"}),"\n",(0,i.jsx)(t.p,{children:"We can perform the vector searches using two types of data:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Text: By text description, we can find images similar to the text description."}),"\n",(0,i.jsx)(t.li,{children:"Image: By using an image, we can find images similar to the provided image."}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# <tab: Text>\nitem = Document({compatible_key: "Find a black dog"})\n'})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"# <tab: Image>\nfrom IPython.display import display\nsearch_image = data[0]\ndisplay(search_image)\nitem = Document({indexing_key: search_image})\n"})}),"\n",(0,i.jsx)(t.p,{children:"Once we have this search target, we can execute a search as follows."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"select = db['docs'].like(item, vector_index=vector_index_name, n=5).select()\nresults = list(db.execute(select))\n"})}),"\n",(0,i.jsx)(t.h2,{id:"visualize-results",children:"Visualize Results"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"from IPython.display import display\nfor result in results:\n    display(result[indexing_key])\n"})}),"\n",(0,i.jsxs)(t.h2,{id:"create-a-template",children:["Create a ",(0,i.jsx)(t.code,{children:"Template"})]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"from superduper import Template\n\ntemplate = Template(\n    'image-vector-search',\n    template=application,\n    substitutions={'docs': 'table'},\n)\n\ntemplate.export('.')\n"})})]})}function p(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>d});var i=n(6540);const o={},a=i.createContext(o);function r(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);