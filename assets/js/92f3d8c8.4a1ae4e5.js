"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[7702],{8601:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>d,toc:()=>c});var a=n(4848),r=n(8453),s=n(1470),l=n(9365);const i={sidebar_label:"Fine tune LLM on database",filename:"build.md"},o="Fine tune LLM on database",d={id:"templates/llm_finetuning",title:"llm_finetuning",description:"Connect to superduper",source:"@site/docs/templates/llm_finetuning.md",sourceDirName:"templates",slug:"/templates/llm_finetuning",permalink:"/docs/templates/llm_finetuning",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/docs/templates/llm_finetuning.md",tags:[],version:"current",frontMatter:{sidebar_label:"Fine tune LLM on database",filename:"build.md"},sidebar:"tutorialSidebar",previous:{title:"Templates",permalink:"/docs/category/templates"},next:{title:"Multimodal vector search - Image",permalink:"/docs/templates/multimodal_image_search"}},u={},c=[{value:"Connect to superduper",id:"connect-to-superduper",level:2},{value:"Get LLM Finetuning Data",id:"get-llm-finetuning-data",level:2},{value:"Insert simple data",id:"insert-simple-data",level:2},{value:"Select a Model",id:"select-a-model",level:2},{value:"Build A Trainable LLM",id:"build-a-trainable-llm",level:2},{value:"Load the trained model",id:"load-the-trained-model",level:2}];function p(e){const t={admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"fine-tune-llm-on-database",children:"Fine tune LLM on database"}),"\n",(0,a.jsx)(t.h2,{id:"connect-to-superduper",children:"Connect to superduper"}),"\n",(0,a.jsx)(t.admonition,{type:"note",children:(0,a.jsx)(t.p,{children:'Note that this is only relevant if you are running superduper in development mode.\nOtherwise refer to "Configuring your production system".'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from superduper import superduper\n\ndb = superduper('mongomock:///test_db')\n"})}),"\n",(0,a.jsx)(t.h2,{id:"get-llm-finetuning-data",children:"Get LLM Finetuning Data"}),"\n",(0,a.jsx)(t.p,{children:"The following are examples of training data in different formats."}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(l.A,{value:"Text",label:"Text",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from datasets import load_dataset\nfrom superduper.base.document import Document\ndataset_name = "timdettmers/openassistant-guanaco"\ndataset = load_dataset(dataset_name)\n\ntrain_dataset = dataset["train"]\neval_dataset = dataset["test"]\n\ntrain_documents = [\n    Document({**example, "_fold": "train"})\n    for example in train_dataset\n]\neval_documents = [\n    Document({**example, "_fold": "valid"})\n    for example in eval_dataset\n]\n\ndatas = train_documents + eval_documents        \n'})})}),(0,a.jsx)(l.A,{value:"Prompt-Response",label:"Prompt-Response",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from datasets import load_dataset\n\nfrom superduper.base.document import Document\ndataset_name = "mosaicml/instruct-v3"\ndataset = load_dataset(dataset_name)\n\ntrain_dataset = dataset["train"]\neval_dataset = dataset["test"]\n\ntrain_documents = [\n    Document({**example, "_fold": "train"})\n    for example in train_dataset\n]\neval_documents = [\n    Document({**example, "_fold": "valid"})\n    for example in eval_dataset\n]\n\ndatas = train_documents + eval_documents        \n'})})}),(0,a.jsx)(l.A,{value:"Chat",label:"Chat",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from datasets import load_dataset\nfrom superduper.base.document import Document\ndataset_name = "philschmid/dolly-15k-oai-style"\ndataset = load_dataset(dataset_name)[\'train\'].train_test_split(0.9)\n\ntrain_dataset = dataset["train"]\neval_dataset = dataset["test"]\n\ntrain_documents = [\n    Document({**example, "_fold": "train"})\n    for example in train_dataset\n]\neval_documents = [\n    Document({**example, "_fold": "valid"})\n    for example in eval_dataset\n]\n\ndatas = train_documents + eval_documents        \n'})})})]}),"\n",(0,a.jsx)(t.p,{children:"We can define different training parameters to handle this type of data."}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(l.A,{value:"Text",label:"Text",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# Function for transformation after extracting data from the database\ntransform = None\nkey = ('text')\ntraining_kwargs=dict(dataset_text_field=\"text\")        \n"})})}),(0,a.jsx)(l.A,{value:"Prompt-Response",label:"Prompt-Response",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# Function for transformation after extracting data from the database\ndef transform(prompt, response):\n    return {'text': prompt + response + \"</s>\"}\n\nkey = ('prompt', 'response')\ntraining_kwargs=dict(dataset_text_field=\"text\")        \n"})})}),(0,a.jsx)(l.A,{value:"Chat",label:"Chat",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# Function for transformation after extracting data from the database\ntransform = None\n\nkey = ('messages')\ntraining_kwargs=None        \n"})})})]}),"\n",(0,a.jsx)(t.p,{children:"Example input_text and output_text"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(l.A,{value:"Text",label:"Text",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'data = datas[0]\ninput_text, output_text = data["text"].rsplit("### Assistant: ", maxsplit=1)\ninput_text += "### Assistant: "\noutput_text = output_text.rsplit("### Human:")[0]\nprint("Input: --------------")\nprint(input_text)\nprint("Response: --------------")\nprint(output_text)        \n'})})}),(0,a.jsx)(l.A,{value:"Prompt-Response",label:"Prompt-Response",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'data = datas[0]\ninput_text = data["prompt"]\noutput_text = data["response"]\nprint("Input: --------------")\nprint(input_text)\nprint("Response: --------------")\nprint(output_text)        \n'})})}),(0,a.jsx)(l.A,{value:"Chat",label:"Chat",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'data = datas[0]\nmessages = data["messages"]\ninput_text = messages[:-1]\noutput_text = messages[-1]["content"]\nprint("Input: --------------")\nprint(input_text)\nprint("Response: --------------")\nprint(output_text)        \n'})})})]}),"\n",(0,a.jsx)(t.h2,{id:"insert-simple-data",children:"Insert simple data"}),"\n",(0,a.jsx)(t.p,{children:"After turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from superduper import Document\n\ntable_or_collection = db['docs']\n\nids = db.execute(table_or_collection.insert([Document(data) for data in datas]))\nselect = table_or_collection.select()\n"})}),"\n",(0,a.jsx)(t.h2,{id:"select-a-model",children:"Select a Model"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'model_name = "facebook/opt-125m"\nmodel_kwargs = dict()\ntokenizer_kwargs = dict()\n\n# or \n# model_name = "mistralai/Mistral-7B-Instruct-v0.2"\n# token = "hf_xxxx"\n# model_kwargs = dict(token=token)\n# tokenizer_kwargs = dict(token=token)\n'})}),"\n",(0,a.jsx)(t.h2,{id:"build-a-trainable-llm",children:"Build A Trainable LLM"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Create an LLM Trainer for training"})}),"\n",(0,a.jsxs)(t.p,{children:["The parameters of this LLM Trainer are basically the same as ",(0,a.jsx)(t.code,{children:"transformers.TrainingArguments"}),", but some additional parameters have been added for easier training setup."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from superduper_transformers import LLM, LLMTrainer\n\ntrainer = LLMTrainer(\n    identifier="llm-finetune-trainer",\n    output_dir="output/finetune",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    save_total_limit=3,\n    logging_steps=10,\n    evaluation_strategy="steps",\n    save_steps=100,\n    eval_steps=100,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    max_seq_length=512,\n    key=key,\n    select=select,\n    transform=transform,\n    training_kwargs=training_kwargs,\n)\n'})}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(l.A,{value:"Lora",label:"Lora",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"trainer.use_lora = True        \n"})})}),(0,a.jsx)(l.A,{value:"QLora",label:"QLora",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"trainer.use_lora = True\ntrainer.bits = 4        \n"})})}),(0,a.jsx)(l.A,{value:"Deepspeed",label:"Deepspeed",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'!pip install deepspeed\ndeepspeed = {\n    "train_batch_size": "auto",\n    "train_micro_batch_size_per_gpu": "auto",\n    "gradient_accumulation_steps": "auto",\n    "zero_optimization": {\n        "stage": 2,\n    },\n}\ntrainer.use_lora = True\ntrainer.bits = 4\ntrainer.deepspeed = deepspeed        \n'})})}),(0,a.jsx)(l.A,{value:"Multi-GPUS",label:"Multi-GPUS",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"trainer.use_lora = True\ntrainer.bits = 4\ntrainer.num_gpus = 2        \n"})})})]}),"\n",(0,a.jsx)(t.p,{children:"Create a trainable LLM model and add it to the database, then the training task will run automatically."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'llm = LLM(\n    identifier="llm",\n    model_name_or_path=model_name,\n    trainer=trainer,\n    model_kwargs=model_kwargs,\n    tokenizer_kwargs=tokenizer_kwargs,\n)\n\ndb.apply(llm)\n'})}),"\n",(0,a.jsx)(t.h2,{id:"load-the-trained-model",children:"Load the trained model"}),"\n",(0,a.jsx)(t.p,{children:"There are two methods to load a trained model:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Load the model directly"}),": This will load the model with the best metrics (if the transformers' best model save strategy is set) or the last version of the model."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Use a specified checkpoint"}),": This method downloads the specified checkpoint, then initializes the base model, and finally merges the checkpoint with the base model. This approach supports custom operations such as resetting flash_attentions, model quantization, etc., during initialization."]}),"\n"]}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(l.A,{value:"Load Trained Model Directly",label:"Load Trained Model Directly",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'llm = db.load("model", "llm")        \n'})})}),(0,a.jsx)(l.A,{value:"Use a specified checkpoint",label:"Use a specified checkpoint",default:!0,children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from superduper_transformers import LLM\n\nexperiment_id = db.show("checkpoint")[-1]\nversion = None # None means the last checkpoint\ncheckpoint = db.load("checkpoint", experiment_id, version=version)\nllm = LLM(\n    identifier="llm",\n    model_name_or_path=model_name,\n    adapter_id=checkpoint,\n    model_kwargs=dict(load_in_4bit=True)\n)        \n'})})})]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"llm.predict(input_text, max_new_tokens=200)\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from superduper import Template\n\nt = Template('llm-finetune', template=llm, substitutions={'docs': 'collection', model_name: 'model_name'})\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"t.export('.')\n"})})]})}function m(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},9365:(e,t,n)=>{n.d(t,{A:()=>l});n(6540);var a=n(870);const r={tabItem:"tabItem_Ymn6"};var s=n(4848);function l(e){let{children:t,hidden:n,className:l}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,l),hidden:n,children:t})}},1470:(e,t,n)=>{n.d(t,{A:()=>y});var a=n(6540),r=n(870),s=n(3104),l=n(6347),i=n(205),o=n(7485),d=n(1682),u=n(9466);function c(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:t,children:n}=e;return(0,a.useMemo)((()=>{const e=t??function(e){return c(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:r}}=e;return{value:t,label:n,attributes:a,default:r}}))}(n);return function(e){const t=(0,d.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function m(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function h(e){let{queryString:t=!1,groupId:n}=e;const r=(0,l.W6)(),s=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,o.aZ)(s),(0,a.useCallback)((e=>{if(!s)return;const t=new URLSearchParams(r.location.search);t.set(s,e),r.replace({...r.location,search:t.toString()})}),[s,r])]}function f(e){const{defaultValue:t,queryString:n=!1,groupId:r}=e,s=p(e),[l,o]=(0,a.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:s}))),[d,c]=h({queryString:n,groupId:r}),[f,x]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,s]=(0,u.Dv)(n);return[r,(0,a.useCallback)((e=>{n&&s.set(e)}),[n,s])]}({groupId:r}),_=(()=>{const e=d??f;return m({value:e,tabValues:s})?e:null})();(0,i.A)((()=>{_&&o(_)}),[_]);return{selectedValue:l,selectValue:(0,a.useCallback)((e=>{if(!m({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);o(e),c(e),x(e)}),[c,x,s]),tabValues:s}}var x=n(2303);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=n(4848);function b(e){let{className:t,block:n,selectedValue:a,selectValue:l,tabValues:i}=e;const o=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.a_)(),u=e=>{const t=e.currentTarget,n=o.indexOf(t),r=i[n].value;r!==a&&(d(t),l(r))},c=e=>{let t=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const n=o.indexOf(e.currentTarget)+1;t=o[n]??o[0];break}case"ArrowLeft":{const n=o.indexOf(e.currentTarget)-1;t=o[n]??o[o.length-1];break}}t?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},t),children:i.map((e=>{let{value:t,label:n,attributes:s}=e;return(0,g.jsx)("li",{role:"tab",tabIndex:a===t?0:-1,"aria-selected":a===t,ref:e=>o.push(e),onKeyDown:c,onClick:u,...s,className:(0,r.A)("tabs__item",_.tabItem,s?.className,{"tabs__item--active":a===t}),children:n??t},t)}))})}function v(e){let{lazy:t,children:n,selectedValue:r}=e;const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===r));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:s.map(((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==r})))})}function j(e){const t=f(e);return(0,g.jsxs)("div",{className:(0,r.A)("tabs-container",_.tabList),children:[(0,g.jsx)(b,{...e,...t}),(0,g.jsx)(v,{...e,...t})]})}function y(e){const t=(0,x.A)();return(0,g.jsx)(j,{...e,children:c(e.children)},String(t))}},8453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>i});var a=n(6540);const r={},s=a.createContext(r);function l(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),a.createElement(s.Provider,{value:t},e.children)}}}]);