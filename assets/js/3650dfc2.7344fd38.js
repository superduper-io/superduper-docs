"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[410],{2869:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>d});var r=t(4848),s=t(8453);const a={},i="PDF RAG",o={id:"templates/pdf_rag",title:"PDF RAG",description:"This is a PDF-based RAG application. While answering questions, it accesses relevant information from the PDF and displays the corresponding paragraphs in the form of images.",source:"@site/docs/templates/pdf_rag.md",sourceDirName:"templates",slug:"/templates/pdf_rag",permalink:"/docs/templates/pdf_rag",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/docs/templates/pdf_rag.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Multimodal vector search - Video",permalink:"/docs/templates/multimodal_video_search"},next:{title:"Retrieval augmented generation",permalink:"/docs/templates/rag"}},l={},d=[{value:"Create a table to store PDFs.",id:"create-a-table-to-store-pdfs",level:2},{value:"Split the PDF file into images for later result display",id:"split-the-pdf-file-into-images-for-later-result-display",level:2},{value:"Build a chunks model and return chunk results with coordinate information.",id:"build-a-chunks-model-and-return-chunk-results-with-coordinate-information",level:2},{value:"Build a vector index for vector search",id:"build-a-vector-index-for-vector-search",level:2},{value:"Create a plugin",id:"create-a-plugin",level:2},{value:"Create a RAG model",id:"create-a-rag-model",level:2},{value:"Create template",id:"create-template",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"pdf-rag",children:"PDF RAG"}),"\n",(0,r.jsx)(n.p,{children:"This is a PDF-based RAG application. While answering questions, it accesses relevant information from the PDF and displays the corresponding paragraphs in the form of images."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"APPLY = False\nCOLLECTION_NAME = '<var:table_name>' if not APPLY else 'sample_pdf_rag'\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import superduper, CFG\nCFG.bytes_encoding = 'str'\nCFG.native_json = False\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'db = superduper("mongomock://")\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def getter():\n    import subprocess\n    subprocess.run(['curl', '-O', 'https://superduperdb-public-demo.s3.amazonaws.com/pdfs.zip'])\n    subprocess.run(['unzip', '-o', 'pdfs.zip'])\n    subprocess.run(['rm', 'pdfs.zip'])\n    pdf_folder = \"pdfs\"\n    pdf_names = [pdf for pdf in os.listdir(pdf_folder) if pdf.endswith(\".pdf\")]\n    pdf_paths = [os.path.join(pdf_folder, pdf) for pdf in pdf_names]\n    data = [{\"url\": pdf_path, \"file\": pdf_path} for pdf_path in pdf_paths]\n    return data\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    data = getter()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"create-a-table-to-store-pdfs",children:"Create a table to store PDFs."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nfrom superduper import Schema, Table\nfrom superduper.components.datatype import file_lazy\n\nschema = Schema(identifier=\"myschema\", fields={'url': 'str', 'file': file_lazy})\ntable = Table(identifier=COLLECTION_NAME, schema=schema)\n\nif APPLY:\n    db.apply(table, force=True)\n    db[COLLECTION_NAME].insert(data).execute()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"split-the-pdf-file-into-images-for-later-result-display",children:"Split the PDF file into images for later result display"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from superduper import ObjectModel, logging\nfrom pdf2image import convert_from_path\nimport os\n\n\ndef split_image(pdf_path):\n    logging.info(f"Splitting images from {pdf_path}")\n\n    image_folders = "data/pdf-images"\n    pdf_name = os.path.basename(pdf_path)\n    images = convert_from_path(pdf_path)\n    logging.info(f"Number of images: {len(images)}")\n\n    image_folder = os.path.join(image_folders, pdf_name)\n    if not os.path.exists(image_folder):\n        os.makedirs(image_folder)\n\n    data = []\n    for i, image in enumerate(images):\n        path = os.path.join(image_folder, f"{i}.jpg")\n        image.save(os.path.join(path))\n        data.append(path)\n    return data\n\n\nmodel_split_image = ObjectModel(\n    identifier="split_image",\n    object=split_image,\n    datatype=file_lazy,\n)\n\nlistener_split_image = model_split_image.to_listener(\n    key="file",\n    select=db[COLLECTION_NAME].find(),\n    flatten=True,\n)\n\nif APPLY:\n    db.apply(listener_split_image, force=True)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"build-a-chunks-model-and-return-chunk-results-with-coordinate-information",children:"Build a chunks model and return chunk results with coordinate information."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def remove_sidebars(elements):\n    import re\n    from collections import defaultdict\n\n    from unstructured.documents.elements import ElementType\n\n    if not elements:\n        return elements\n    points_groups = defaultdict(list)\n    min_x = 99999999\n    max_x = 0\n    e2index = {e.id: i for i, e in enumerate(elements)}\n    for e in elements:\n        x_l = int(e.metadata.coordinates.points[0][0])\n        x_r = int(e.metadata.coordinates.points[2][0])\n        points_groups[(x_l, x_r)].append(e)\n        min_x = min(min_x, x_l)\n        max_x = max(max_x, x_r)\n    sidebars_elements = set()\n    for (x_l, x_r), es in points_groups.items():\n        first_id = e2index[es[0].id]\n        last_id = e2index[es[-1].id]\n        on_left = first_id == 0 and x_l == min_x\n        on_right = (last_id == len(elements) - 2) and x_r == max_x\n        loc_match = [on_left, on_right]\n        total_text = "".join(map(str, es))\n        condiction = [\n            any(loc_match),\n            len(es) >= 3,\n            re.findall("^[A-Z\\s\\d,]+$", total_text),\n        ]\n        if not all(condiction):\n            continue\n        sidebars_elements.update(map(lambda x: x.id, es))\n        if on_left:\n            check_page_num_e = elements[last_id + 1]\n        else:\n            check_page_num_e = elements[-1]\n        if (\n            check_page_num_e.category == ElementType.UNCATEGORIZED_TEXT\n            and check_page_num_e.text.strip().isalnum()\n        ):\n            sidebars_elements.add(check_page_num_e.id)\n\n    elements = [e for e in elements if e.id not in sidebars_elements]\n    return elements\n\n\ndef remove_annotation(elements):\n    from collections import Counter\n\n    from unstructured.documents.elements import ElementType\n\n    page_num = max(e.metadata.page_number for e in elements)\n    un_texts_counter = Counter(\n        [e.text for e in elements if e.category == ElementType.UNCATEGORIZED_TEXT]\n    )\n    rm_text = set()\n    for text, count in un_texts_counter.items():\n        if count / page_num >= 0.5:\n            rm_text.add(text)\n    elements = [e for e in elements if e.text not in rm_text]\n    return elements\n\n\ndef create_chunk_and_metadatas(page_elements, stride=3, window=10):\n    page_elements = remove_sidebars(page_elements)\n    for index, page_element in enumerate(page_elements):\n        page_element.metadata.num = index\n    datas = []\n    for i in range(0, len(page_elements), stride):\n        windown_elements = page_elements[i : i + window]\n        chunk = "\\n".join([e.text for e in windown_elements])\n        source_elements = [e.to_dict() for e in windown_elements]\n        datas.append(\n            {\n                "txt": chunk,\n                "source_elements": source_elements,\n            }\n        )\n    return datas\n\n\ndef get_chunks(pdf):\n    from collections import defaultdict\n\n    from unstructured.documents.coordinates import RelativeCoordinateSystem\n    from unstructured.partition.pdf import partition_pdf\n\n    elements = partition_pdf(pdf)\n    elements = remove_annotation(elements)\n\n    pages_elements = defaultdict(list)\n    for element in elements:\n        element.convert_coordinates_to_new_system(\n            RelativeCoordinateSystem(), in_place=True\n        )\n        pages_elements[element.metadata.page_number].append(element)\n\n    all_chunks_and_links = sum(\n        [\n            create_chunk_and_metadatas(page_elements)\n            for _, page_elements in pages_elements.items()\n        ],\n        [],\n    )\n    return all_chunks_and_links\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from superduper.components.schema import FieldType\n\nmodel_chunk = ObjectModel(\n    identifier="chunk",\n    object=get_chunks,\n    datatype=FieldType(identifier="json")\n)\n\nlistener_chunk = model_chunk.to_listener(\n    key="file",\n    select=db[COLLECTION_NAME].select(),\n    flatten=True,\n)\n\nif APPLY:\n    db.apply(listener_chunk, force=True)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"build-a-vector-index-for-vector-search",children:"Build a vector index for vector search"}),"\n",(0,r.jsx)(n.p,{children:"OpenAI:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nfrom superduper.components.vector_index import sqlvector\n\nfrom superduper_openai import OpenAIEmbedding\n\nopenai_embedding = OpenAIEmbedding(identifier='text-embedding-ada-002' , datatype=sqlvector(shape=(1536,)))\n"})}),"\n",(0,r.jsx)(n.p,{children:"Sentence-transformers:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import sentence_transformers\nfrom superduper_sentence_transformers import SentenceTransformer\n\nsentence_transformers_embedding = SentenceTransformer(\n    identifier="sentence-transformers-embedding",\n    model="BAAI/bge-small-en",\n    datatype=sqlvector(shape=(1024,)),\n    postprocess=lambda x: x.tolist(),\n    predict_kwargs={"show_progress_bar": True},\n)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper.components.model import ModelRouter\nfrom superduper.components.vector_index import sqlvector\n\nmodel_embedding = ModelRouter(\n    'embedding',\n    models={'openai': openai_embedding, 'sentence_transformers': sentence_transformers_embedding},\n    model='<var:embedding_model>' if not APPLY else 'openai',\n    example='this is a test',\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from superduper_openai.model import OpenAIEmbedding\nfrom superduper import VectorIndex\n\nlistener_embedding = model_embedding.to_listener(\n    key=f"{listener_chunk.outputs}.txt",\n    select=db[listener_chunk.outputs].select(),\n)\n\nvector_index = VectorIndex(\n    identifier="vector-index",\n    indexing_listener=listener_embedding,\n)\n\nif APPLY:\n    db.apply(vector_index, force=True)\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"create-a-plugin",children:"Create a plugin"}),"\n",(0,r.jsx)(n.p,{children:"When applying the processor, saves the plugin in the database, thereby saving the related dependencies as well."}),"\n",(0,r.jsx)(n.p,{children:"The processor will integrate the returned chunks information with the images, and return a visualized image.\u200b"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from superduper import Plugin\nfrom utils import Processor\n\nprocessor = Processor(\n    identifier="processor",\n    db=db,\n    chunk_key=listener_chunk.outputs,\n    split_image_key=listener_split_image.outputs,\n    plugins=[Plugin(path="./utils.py")],\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"create-a-rag-model",children:"Create a RAG model"}),"\n",(0,r.jsx)(n.p,{children:"Create a RAG model to perform retrieval-augmented generation (RAG) and return the results."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from superduper import Model, logging\n\n\nclass Rag(Model):\n    llm_model: Model\n    vector_index_name: str\n    prompt_template: str\n    processor: None | Model = None\n\n    def __post_init__(self, *args, **kwargs):\n        assert "{context}" in self.prompt_template, \'The prompt_template must include "{context}"\'\n        assert "{query}" in self.prompt_template, \'The prompt_template must include "{query}"\'\n        super().__post_init__(*args, **kwargs)\n\n    def init(self, db=None):\n        db = db or self.db\n        self.vector_index = self.db.load("vector_index", self.vector_index_name)\n        super().init(db=db)\n        \n    \n    def predict(self, query, top_k=5, format_result=False):\n        vector_search_out = self.vector_search(query, top_k=top_k)\n        key = self.vector_index.indexing_listener.key\n        context = "\\n\\n---\\n\\n".join([x[key] for x in vector_search_out])\n        \n        prompt = self.prompt_template.format(context=context, query=query)\n        output = self.llm_model.predict(prompt)\n        result = {\n            "answer": output,\n            "docs": vector_search_out,\n        }\n        if format_result and self.processor:\n            result["images"] = list(self.processor.predict(\n                vector_search_out,\n                match_text=output,\n            ))\n        return result\n\n    def vector_search(self, query, top_k=5, format_result=False):\n        logging.info(f"Vector search query: {query}")\n        select = self.db[self.vector_index.indexing_listener.select.table].like(\n            {self.vector_index.indexing_listener.key:query},\n            vector_index=self.vector_index.identifier, \n            n=top_k,\n        ).select()\n        out = select.execute()\n        if out:\n            out = sorted(out, key=lambda x: x["score"], reverse=True)\n        return out\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper_openai import OpenAIChatCompletion\n\nllm_openai = OpenAIChatCompletion(identifier='llm-openai', model='gpt-3.5-turbo')\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper_anthropic import AnthropicCompletions\n\npredict_kwargs = {\n    \"max_tokens\": 1024,\n    \"temperature\": 0.8,\n}\n\nllm_anthropic = AnthropicCompletions(identifier='llm-anthropic', model='claude-2.1', predict_kwargs=predict_kwargs)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from superduper_vllm import VllmCompletion\n\npredict_kwargs = {\n    "max_tokens": 1024,\n    "temperature": 0.8,\n}\n\nllm_vllm = VllmCompletion(\n    identifier="llm-vllm",\n    vllm_params={\n        \'model\': \'TheBloke/Mistral-7B-Instruct-v0.2-AWQ\',\n        "gpu_memory_utilization": 0.7,\n        "max_model_len": 1024,\n        "quantization": "awq",\n    },\n    predict_kwargs=predict_kwargs,\n)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"llm = ModelRouter(\n    'llm',\n    models={\n        'openai': llm_openai,\n        'anthropic': llm_anthropic,\n        'vllm': llm_vllm,\n    },\n    model='<var:llm_model>' if not APPLY else 'openai',\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from superduper_openai.model import OpenAIChatCompletion\n\nprompt_template = (\n    "The following is a document and question\\n"\n    "Only provide a very concise answer\\n"\n    "Context:\\n\\n"\n    "{context}\\n\\n"\n    "Here\'s the question:{query}\\n"\n    "answer:"\n)\n\nrag = Rag(identifier="rag", llm_model=llm, vector_index_name=vector_index.identifier, prompt_template=prompt_template, db=db, processor=processor)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from IPython.display import Image, Markdown, display\n\nif APPLY:\n    db.apply(rag, force=True)\n    result = rag.predict("How to perform Query Optimization?", format_result=True)\n    \n    display(Markdown(result["answer"]))\n    \n    for message, img in result["images"]:\n        display(Markdown(message))\n        display(img)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"create-template",children:"Create template"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Application\n\napp = Application(\n    'pdf-rag',\n    components=[\n        table,\n        listener_split_image,\n        listener_chunk,\n        vector_index,\n        rag\n    ]\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper import Template, CFG, Table\nfrom superduper.components.dataset import RemoteData\n\ntemplate = Template(\n    'pdf-rag',\n    template=app,\n    substitutions={prompt_template: 'prompt_template', COLLECTION_NAME: 'table_name'},\n    template_variables=['table_name', 'prompt_template', 'llm_model', 'embedding_model'],\n    default_table=Table(\n        'sample_pdf_rag',\n        schema=Schema(\n            'sample_pdf_rag/schema',\n            fields={\"url\": \"str\", \"file\": file_lazy}\n        ),\n        data=RemoteData('sample_pdfs', getter=getter),\n    ),\n    types={\n        'prompt_template':{\n            'type': 'str',\n            'default': prompt_template\n        },\n        'table_name': {\n            'type': 'str',\n            'default': 'sample_pdf_rag'\n        },\n        'llm_model': {\n            'type': 'str',\n            'choices': ['openai', 'anthropic', 'vllm'],\n            'default': 'openai',\n        },\n        'embedding_model': {\n            'type': 'str',\n            'choices': ['openai', 'sentence_transformers'],\n            'default': 'openai',\n        },\n    }\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'template.export(".")\n'})})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var r=t(6540);const s={},a=r.createContext(s);function i(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);