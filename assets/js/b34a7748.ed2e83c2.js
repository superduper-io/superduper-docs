"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[2297],{1394:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>d,toc:()=>c});var r=t(4848),o=t(8453),a=t(1470),s=t(9365);const i={sidebar_label:"Build multimodal embedding models",filename:"build_multimodal_embedding_models.md"},l="Build multimodal embedding models",d={id:"building_blocks/build_multimodal_embedding_models",title:"build_multimodal_embedding_models",description:"Some embedding models such as CLIP come in pairs of model and compatible_model.",source:"@site/content/building_blocks/build_multimodal_embedding_models.md",sourceDirName:"building_blocks",slug:"/building_blocks/build_multimodal_embedding_models",permalink:"/docs/building_blocks/build_multimodal_embedding_models",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/content/building_blocks/build_multimodal_embedding_models.md",tags:[],version:"current",frontMatter:{sidebar_label:"Build multimodal embedding models",filename:"build_multimodal_embedding_models.md"},sidebar:"tutorialSidebar",previous:{title:"Build image embedding model",permalink:"/docs/building_blocks/build_image_embedding_model"},next:{title:"Build LLM",permalink:"/docs/building_blocks/build_llm"}},u={},c=[];function m(e){const n={a:"a",code:"code",h1:"h1",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"build-multimodal-embedding-models",children:"Build multimodal embedding models"}),"\n",(0,r.jsxs)(n.p,{children:["Some embedding models such as ",(0,r.jsx)(n.a,{href:"https://github.com/openai/CLIP",children:"CLIP"})," come in pairs of ",(0,r.jsx)(n.code,{children:"model"})," and ",(0,r.jsx)(n.code,{children:"compatible_model"}),".\nOtherwise:"]}),"\n",(0,r.jsxs)(a.A,{children:[(0,r.jsx)(s.A,{value:"Text",label:"Text",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduper_sentence_transformers import SentenceTransformer\n\n# Load the pre-trained sentence transformer model\nmodel = SentenceTransformer(\n    identifier='all-MiniLM-L6-v2',\n    postprocess=lambda x: x.tolist(),\n)        \n"})})}),(0,r.jsx)(s.A,{value:"Image",label:"Image",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from torchvision import transforms\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nimport warnings\n\n# Import custom modules\nfrom superduper_torch import TorchModel, tensor\n\n# Define a series of image transformations using torchvision.transforms.Compose\nt = transforms.Compose([\n    transforms.Resize((224, 224)),   # Resize the input image to 224x224 pixels (must same as here)\n    transforms.CenterCrop((224, 224)),  # Perform a center crop on the resized image\n    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the tensor with specified mean and standard deviation\n])\n\n# Define a preprocess function that applies the defined transformations to an input image\ndef preprocess(x):\n    try:\n        return t(x)\n    except Exception as e:\n        # If an exception occurs during preprocessing, issue a warning and return a tensor of zeros\n        warnings.warn(str(e))\n        return torch.zeros(3, 224, 224)\n\n# Load the pre-trained ResNet-50 model from torchvision\nresnet50 = models.resnet50(pretrained=True)\n\n# Extract all layers of the ResNet-50 model except the last one\nmodules = list(resnet50.children())[:-1]\nresnet50 = nn.Sequential(*modules)\n\n# Create a TorchModel instance with the ResNet-50 model, preprocessing function, and postprocessing lambda\nmodel = TorchModel(\n    identifier='resnet50',\n    preprocess=preprocess,\n    object=resnet50,\n    postprocess=lambda x: x[:, 0, 0],  # Postprocess by extracting the top-left element of the output tensor\n    datatype=tensor(dtype='float', shape=(2048,))  # Specify the encoder configuration\n)        \n"})})}),(0,r.jsx)(s.A,{value:"Text-Image",label:"Text-Image",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"!pip install git+https://github.com/openai/CLIP.git\nimport clip\nfrom superduper import vector\nfrom superduper_torch import TorchModel\n\n# Load the CLIP model and obtain the preprocessing function\nmodel, preprocess = clip.load(\"ViT-B/32\", device='cpu')\n\n# Define a vector with shape (1024,)\n\noutput_datatpye = vector(shape=(1024,))\n\n# Create a TorchModel for text encoding\ncompatible_model = TorchModel(\n    identifier='clip_text', # Unique identifier for the model\n    object=model, # CLIP model\n    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n    datatype=output_datatpye,  # Vector encoder with shape (1024,)\n    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n)\n\n# Create a TorchModel for visual encoding\nmodel = TorchModel(\n    identifier='clip_image',  # Unique identifier for the model\n    object=model.visual,  # Visual part of the CLIP model    \n    preprocess=preprocess, # Visual preprocessing using CLIP\n    postprocess=lambda x: x.tolist(), # Convert the output to a list \n    datatype=output_datatpye, # Vector encoder with shape (1024,)\n)        \n"})})}),(0,r.jsx)(s.A,{value:"Audio",label:"Audio",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"!pip install librosa\nimport librosa\nimport numpy as np\nfrom superduper import ObjectModel\nfrom superduper import vector\n\ndef audio_embedding(audio_file):\n    # Load the audio file\n    MAX_SIZE= 10000\n    y, sr = librosa.load(audio_file)\n    y = y[:MAX_SIZE]\n    mfccs = librosa.feature.mfcc(y=y, sr=44000, n_mfcc=1)\n    mfccs =  mfccs.squeeze().tolist()\n    return mfccs\n\nif not get_chunking_datatype:\n    e =  vector(shape=(1000,))\nelse:\n    e = get_chunking_datatype(1000)\n\nmodel= ObjectModel(identifier='my-model-audio', object=audio_embedding, datatype=e)        \n"})})})]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},9365:(e,n,t)=>{t.d(n,{A:()=>s});t(6540);var r=t(870);const o={tabItem:"tabItem_Ymn6"};var a=t(4848);function s(e){let{children:n,hidden:t,className:s}=e;return(0,a.jsx)("div",{role:"tabpanel",className:(0,r.A)(o.tabItem,s),hidden:t,children:n})}},1470:(e,n,t)=>{t.d(n,{A:()=>j});var r=t(6540),o=t(870),a=t(3104),s=t(6347),i=t(205),l=t(7485),d=t(1682),u=t(9466);function c(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,r.useMemo)((()=>{const e=n??function(e){return c(e).map((e=>{let{props:{value:n,label:t,attributes:r,default:o}}=e;return{value:n,label:t,attributes:r,default:o}}))}(t);return function(e){const n=(0,d.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:t}=e;const o=(0,s.W6)(),a=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(a),(0,r.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(o.location.search);n.set(a,e),o.replace({...o.location,search:n.toString()})}),[a,o])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:o}=e,a=m(e),[s,l]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const r=t.find((e=>e.default))??t[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:a}))),[d,c]=h({queryString:t,groupId:o}),[f,b]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[o,a]=(0,u.Dv)(t);return[o,(0,r.useCallback)((e=>{t&&a.set(e)}),[t,a])]}({groupId:o}),g=(()=>{const e=d??f;return p({value:e,tabValues:a})?e:null})();(0,i.A)((()=>{g&&l(g)}),[g]);return{selectedValue:s,selectValue:(0,r.useCallback)((e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),c(e),b(e)}),[c,b,a]),tabValues:a}}var b=t(2303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var _=t(4848);function v(e){let{className:n,block:t,selectedValue:r,selectValue:s,tabValues:i}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,a.a_)(),u=e=>{const n=e.currentTarget,t=l.indexOf(n),o=i[t].value;o!==r&&(d(n),s(o))},c=e=>{let n=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,_.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":t},n),children:i.map((e=>{let{value:n,label:t,attributes:a}=e;return(0,_.jsx)("li",{role:"tab",tabIndex:r===n?0:-1,"aria-selected":r===n,ref:e=>l.push(e),onKeyDown:c,onClick:u,...a,className:(0,o.A)("tabs__item",g.tabItem,a?.className,{"tabs__item--active":r===n}),children:t??n},n)}))})}function x(e){let{lazy:n,children:t,selectedValue:o}=e;const a=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=a.find((e=>e.props.value===o));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return(0,_.jsx)("div",{className:"margin-top--md",children:a.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==o})))})}function y(e){const n=f(e);return(0,_.jsxs)("div",{className:(0,o.A)("tabs-container",g.tabList),children:[(0,_.jsx)(v,{...e,...n}),(0,_.jsx)(x,{...e,...n})]})}function j(e){const n=(0,b.A)();return(0,_.jsx)(y,{...e,children:c(e.children)},String(n))}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>i});var r=t(6540);const o={},a=r.createContext(o);function s(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);