"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[3317],{2577:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>s,contentTitle:()=>c,default:()=>h,frontMatter:()=>d,metadata:()=>r,toc:()=>o});var t=n(4848),i=n(8453);const d={sidebar_position:16},c="Working with and inserting large pieces of data",r={id:"execute_api/using_hybrid_storage_to_handle_large_data_blobs",title:"Working with and inserting large pieces of data",description:"Some applications require large data-blobs and objects, which are either larger than the objects which are supported by the underlying database, or which will degrade performance of the database if stored directly. For example:",source:"@site/docs/execute_api/using_hybrid_storage_to_handle_large_data_blobs.md",sourceDirName:"execute_api",slug:"/execute_api/using_hybrid_storage_to_handle_large_data_blobs",permalink:"/docs/next/execute_api/using_hybrid_storage_to_handle_large_data_blobs",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/docs/execute_api/using_hybrid_storage_to_handle_large_data_blobs.md",tags:[],version:"current",sidebarPosition:16,frontMatter:{sidebar_position:16},sidebar:"tutorialSidebar",previous:{title:"(Optional) Setting up tables and encodings",permalink:"/docs/next/execute_api/data_encodings_and_schemas"},next:{title:"Working with external data sources",permalink:"/docs/next/execute_api/referring_to_data_from_diverse_sources"}},s={},o=[{value:"Artifact store reference with <code>encodable=&#39;artifact&#39;</code>",id:"artifact-store-reference-with-encodableartifact",level:2},{value:"Just-in-time loading with <code>encodable=&#39;lazy_artifact&#39;</code>:",id:"just-in-time-loading-with-encodablelazy_artifact",level:2},{value:"Saving files and directories to the artifact store",id:"saving-files-and-directories-to-the-artifact-store",level:2}];function l(e){const a={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.h1,{id:"working-with-and-inserting-large-pieces-of-data",children:"Working with and inserting large pieces of data"}),"\n",(0,t.jsx)(a.p,{children:"Some applications require large data-blobs and objects, which are either larger than the objects which are supported by the underlying database, or which will degrade performance of the database if stored directly. For example:"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"large images"}),"\n",(0,t.jsx)(a.li,{children:"large audio"}),"\n",(0,t.jsx)(a.li,{children:"videos"}),"\n"]}),"\n",(0,t.jsxs)(a.p,{children:["In order to handle such data, Superduper provides a few options when\ncreating a ",(0,t.jsx)(a.code,{children:"DataType"})," via the ",(0,t.jsx)(a.code,{children:"encodable"})," parameter."]}),"\n",(0,t.jsxs)(a.h2,{id:"artifact-store-reference-with-encodableartifact",children:["Artifact store reference with ",(0,t.jsx)(a.code,{children:"encodable='artifact'"})]}),"\n",(0,t.jsxs)(a.p,{children:["When creating a ",(0,t.jsx)(a.code,{children:"DataType"})," with ",(0,t.jsx)(a.code,{children:"encodable='artifact'"}),",\nthe data encoded by the ",(0,t.jsx)(a.code,{children:"DataType"})," is saved to the ",(0,t.jsx)(a.code,{children:"db.artifact_store"}),"\nand a reference in saved in the ",(0,t.jsx)(a.code,{children:"db.databackend"})]}),"\n",(0,t.jsx)(a.p,{children:"For example, if you try the following snippet:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:"import pickle\nimport uuid\nfrom superduper import DataType, Document, superduper, Table, Schema\n\ndb = superduper('mongomock://test', artifact_store='filesystem://./artifacts')\n\ndt = DataType(\n    'my-artifact',\n    encoder=lambda x, info: pickle.dumps(x),\n    decoder=lambda x, info: pickle.loads(x),\n    encodable='artifact',\n)\n\nschema = Schema(identifier='schema', fields={'x': dt})\ntable = Table('my_collection', schema=schema)\n\ndb.apply(table)\n\nmy_id = str(uuid.uuid4())\n\ndb['my_collection'].insert_one(Document({'id': my_id, 'x': 'This is a test'})).execute()\n"})}),"\n",(0,t.jsx)(a.p,{children:"If you now reload the data with this query:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:">>> r = db.execute(db['my_collection'].find_one({'id': my_id}))\n>>> r\nDocument({'id': 'a9a01284-f391-4aaa-9391-318fc38303bb', 'x': 'This is a test', '_fold': 'train', '_id': ObjectId('669fae8ccdaeae826dec4784')})\n"})}),"\n",(0,t.jsxs)(a.p,{children:["You will see that ",(0,t.jsx)(a.code,{children:"r['x']"})," is exactly ",(0,t.jsx)(a.code,{children:"'This is a test'"}),", however,\nwith a native MongoDB query, you will find the data for ",(0,t.jsx)(a.code,{children:"'x'"})," missing:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:">>> db.databackend.conn.test.my_collection.find_one() \n{'id': 'a9a01284-f391-4aaa-9391-318fc38303bb',\n 'x': '&:blob:866cf8526595d3620d6045172fb16d1efefac4b1',\n '_fold': 'train',\n '_schema': 'schema',\n '_builds': {},\n '_files': {},\n '_blobs': {},\n '_id': ObjectId('669fae8ccdaeae826dec4784')}\n"})}),"\n",(0,t.jsxs)(a.p,{children:["This is because the data is stored in the filesystem/ artifact store ",(0,t.jsx)(a.code,{children:"./artifacts"}),".\nYou may verify that with this command:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-bash",children:"iconv -f ISO-8859-1 -t UTF-8 artifacts/866cf8526595d3620d6045172fb16d1efefac4b1\n"})}),"\n",(0,t.jsx)(a.p,{children:"The Superduper query reloads the data and passes it to the query result,\nwithout any user intervention."}),"\n",(0,t.jsxs)(a.h2,{id:"just-in-time-loading-with-encodablelazy_artifact",children:["Just-in-time loading with ",(0,t.jsx)(a.code,{children:"encodable='lazy_artifact'"}),":"]}),"\n",(0,t.jsxs)(a.p,{children:["If you specify ",(0,t.jsx)(a.code,{children:"encodable='lazy_artifact'"}),", then the data\nis only loaded when a user calls the ",(0,t.jsx)(a.code,{children:".unpack()"})," method.\nThis can be useful if the datapoints are very large,\nand should only be loaded when absolutely necessary."]}),"\n",(0,t.jsxs)(a.p,{children:["Try replacing the creation of ",(0,t.jsx)(a.code,{children:"dt"})," with this command:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:"dt = DataType(\n    'my-artifact',\n    encoder=lambda x, info: pickle.dumps(x),\n    decoder=lambda x, info: pickle.loads(x),\n    encodable='lazy_artifact',\n)\n"})}),"\n",(0,t.jsx)(a.p,{children:"and then execute the same lines as before.\nYou will find that:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:">>> r = db.execute(my_collection.find_one({'id': my_id}))\n>>> r\nDocument({'id': 'b2a248c7-e023-4cba-9ac9-fdc92fa77ae3', 'x': LazyArtifact(identifier='', uuid='c0db12ad-2684-4e39-a2ba-2748bd20b193', datatype=DataType(identifier='my-artifact', uuid='6d72b346-b5ec-4d8b-8cba-cddec86937a3', upstream=None, plugins=None, encoder=<function <lambda> at 0x125e33760>, decoder=<function <lambda> at 0x125c4e320>, info=None, shape=None, directory=None, encodable='lazy_artifact', bytes_encoding='Bytes', intermediate_type='bytes', media_type=None), uri=None, x=<EMPTY>), '_fold': 'train', '_id': ObjectId('669faf9dcdaeae826dec4789')})\n>>> r['x'].x\n<EMPTY>\n"})}),"\n",(0,t.jsxs)(a.p,{children:["However, after calling ",(0,t.jsx)(a.code,{children:".unpack(db)"}),":"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:">>> r = r.unpack()\n>>> r['x']\n'This is a test'\n"})}),"\n",(0,t.jsxs)(a.p,{children:["This allows ",(0,t.jsx)(a.code,{children:"superduper"})," to build efficient data-loaders and model loading mechanisms.\nFor example, when saving model data to the artifact-store, the default ",(0,t.jsx)(a.code,{children:"encodable"})," is ",(0,t.jsx)(a.code,{children:"'lazy_artifact'"}),"."]}),"\n",(0,t.jsx)(a.h2,{id:"saving-files-and-directories-to-the-artifact-store",children:"Saving files and directories to the artifact store"}),"\n",(0,t.jsxs)(a.p,{children:["There is an additional mechanism for working with large files. This works\nbetter in certain contexts, such as flexibly saving the results of model training.\nThe following lines copy the file to the ",(0,t.jsx)(a.code,{children:"db.artifact_store"}),".\nWhen data is loaded, the data is copied back over from the artifact-store to\nthe local file-system:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-bash",children:"cp -r test test_copy\n"})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:"schema = Schema(identifier='schema', fields={'x': dt})\ntable = Table('my_collection', schema=schema)\n\ndb.apply(table)\nmy_id = str(uuid.uuid4())\ndb.execute(db['my_collection'].insert_one(Document({'id': my_id, 'x': './test_copy'})))\n"})}),"\n",(0,t.jsx)(a.p,{children:"When reloading data, you will see that only a reference to the data in the artifact-store\nis loaded:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:">>> db.execute(db['my_collection'].find_one({'id': my_id})).unpack()\n{'id': '93eaae04-a48b-4632-94cf-123cdb2c9517',\n 'x': './artifacts/d537309c8e5be28f91b90b97bbb229984935ba4a/test_copy',\n '_fold': 'train',\n '_id': ObjectId('669fb091cdaeae826dec4797')}\n\n"})}),"\n",(0,t.jsxs)(a.p,{children:["Downstream ",(0,t.jsx)(a.code,{children:"Model"})," instances may then explicitly handle the local file from the file\nreference."]})]})}function h(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>c,x:()=>r});var t=n(6540);const i={},d=t.createContext(i);function c(e){const a=t.useContext(d);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:c(e.components),t.createElement(d.Provider,{value:a},e.children)}}}]);