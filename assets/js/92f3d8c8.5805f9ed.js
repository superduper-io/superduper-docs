"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[7702],{8601:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var a=t(4848),s=t(8453);const r={},i="Fine tune LLM on database",o={id:"templates/llm_finetuning",title:"Fine tune LLM on database",description:"Connect to superduper",source:"@site/docs/templates/llm_finetuning.md",sourceDirName:"templates",slug:"/templates/llm_finetuning",permalink:"/docs/templates/llm_finetuning",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/docs/templates/llm_finetuning.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Templates",permalink:"/docs/category/templates"},next:{title:"Multimodal vector search - images",permalink:"/docs/templates/multimodal_image_search"}},d={},l=[{value:"Connect to superduper",id:"connect-to-superduper",level:2},{value:"Get LLM Finetuning Data",id:"get-llm-finetuning-data",level:2},{value:"Insert simple data",id:"insert-simple-data",level:2},{value:"Select a Model",id:"select-a-model",level:2},{value:"Build A Trainable LLM",id:"build-a-trainable-llm",level:2},{value:"Load the trained model",id:"load-the-trained-model",level:2}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"fine-tune-llm-on-database",children:"Fine tune LLM on database"}),"\n",(0,a.jsx)(n.h2,{id:"connect-to-superduper",children:"Connect to superduper"}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsx)(n.p,{children:'Note that this is only relevant if you are running superduper in development mode.\nOtherwise refer to "Configuring your production system".'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import superduper\n\ndb = superduper('mongomock:///test_db')\n"})}),"\n",(0,a.jsx)(n.h2,{id:"get-llm-finetuning-data",children:"Get LLM Finetuning Data"}),"\n",(0,a.jsx)(n.p,{children:"The following are examples of training data in different formats."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# <tab: Text>\nfrom datasets import load_dataset\nfrom superduper.base.document import Document\ndataset_name = "timdettmers/openassistant-guanaco"\ndataset = load_dataset(dataset_name)\n\ntrain_dataset = dataset["train"]\neval_dataset = dataset["test"]\n\ntrain_documents = [\n    Document({**example, "_fold": "train"})\n    for example in train_dataset\n]\neval_documents = [\n    Document({**example, "_fold": "valid"})\n    for example in eval_dataset\n]\n\ndatas = train_documents + eval_documents\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# <tab: Prompt-Response>\nfrom datasets import load_dataset\n\nfrom superduper.base.document import Document\ndataset_name = "mosaicml/instruct-v3"\ndataset = load_dataset(dataset_name)\n\ntrain_dataset = dataset["train"]\neval_dataset = dataset["test"]\n\ntrain_documents = [\n    Document({**example, "_fold": "train"})\n    for example in train_dataset\n]\neval_documents = [\n    Document({**example, "_fold": "valid"})\n    for example in eval_dataset\n]\n\ndatas = train_documents + eval_documents\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# <tab: Chat>\nfrom datasets import load_dataset\nfrom superduper.base.document import Document\ndataset_name = "philschmid/dolly-15k-oai-style"\ndataset = load_dataset(dataset_name)[\'train\'].train_test_split(0.9)\n\ntrain_dataset = dataset["train"]\neval_dataset = dataset["test"]\n\ntrain_documents = [\n    Document({**example, "_fold": "train"})\n    for example in train_dataset\n]\neval_documents = [\n    Document({**example, "_fold": "valid"})\n    for example in eval_dataset\n]\n\ndatas = train_documents + eval_documents\n'})}),"\n",(0,a.jsx)(n.p,{children:"We can define different training parameters to handle this type of data."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# <tab: Text>\n# Function for transformation after extracting data from the database\ntransform = None\nkey = ('text')\ntraining_kwargs=dict(dataset_text_field=\"text\")\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# <tab: Prompt-Response>\n# Function for transformation after extracting data from the database\ndef transform(prompt, response):\n    return {'text': prompt + response + \"</s>\"}\n\nkey = ('prompt', 'response')\ntraining_kwargs=dict(dataset_text_field=\"text\")\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# <tab: Chat>\n# Function for transformation after extracting data from the database\ntransform = None\n\nkey = ('messages')\ntraining_kwargs=None\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example input_text and output_text"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# <tab: Text>\ndata = datas[0]\ninput_text, output_text = data["text"].rsplit("### Assistant: ", maxsplit=1)\ninput_text += "### Assistant: "\noutput_text = output_text.rsplit("### Human:")[0]\nprint("Input: --------------")\nprint(input_text)\nprint("Response: --------------")\nprint(output_text)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# <tab: Prompt-Response>\ndata = datas[0]\ninput_text = data["prompt"]\noutput_text = data["response"]\nprint("Input: --------------")\nprint(input_text)\nprint("Response: --------------")\nprint(output_text)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# <tab: Chat>\ndata = datas[0]\nmessages = data["messages"]\ninput_text = messages[:-1]\noutput_text = messages[-1]["content"]\nprint("Input: --------------")\nprint(input_text)\nprint("Response: --------------")\nprint(output_text)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"insert-simple-data",children:"Insert simple data"}),"\n",(0,a.jsx)(n.p,{children:"After turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import Document\n\ntable_or_collection = db['docs']\n\nids = db.execute(table_or_collection.insert([Document(data) for data in datas]))\nselect = table_or_collection.select()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"select-a-model",children:"Select a Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'model_name = "facebook/opt-125m"\nmodel_kwargs = dict()\ntokenizer_kwargs = dict()\n\n# or \n# model_name = "mistralai/Mistral-7B-Instruct-v0.2"\n# token = "hf_xxxx"\n# model_kwargs = dict(token=token)\n# tokenizer_kwargs = dict(token=token)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"build-a-trainable-llm",children:"Build A Trainable LLM"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Create an LLM Trainer for training"})}),"\n",(0,a.jsxs)(n.p,{children:["The parameters of this LLM Trainer are basically the same as ",(0,a.jsx)(n.code,{children:"transformers.TrainingArguments"}),", but some additional parameters have been added for easier training setup."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from superduper_transformers import LLM, LLMTrainer\n\ntrainer = LLMTrainer(\n    identifier="llm-finetune-trainer",\n    output_dir="output/finetune",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    save_total_limit=3,\n    logging_steps=10,\n    evaluation_strategy="steps",\n    save_steps=100,\n    eval_steps=100,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    max_seq_length=512,\n    key=key,\n    select=select,\n    transform=transform,\n    training_kwargs=training_kwargs,\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# <tab: Lora>\ntrainer.use_lora = True\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# <tab: QLora>\ntrainer.use_lora = True\ntrainer.bits = 4\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# <tab: Deepspeed>\n!pip install deepspeed\ndeepspeed = {\n    "train_batch_size": "auto",\n    "train_micro_batch_size_per_gpu": "auto",\n    "gradient_accumulation_steps": "auto",\n    "zero_optimization": {\n        "stage": 2,\n    },\n}\ntrainer.use_lora = True\ntrainer.bits = 4\ntrainer.deepspeed = deepspeed\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# <tab: Multi-GPUS>\ntrainer.use_lora = True\ntrainer.bits = 4\ntrainer.num_gpus = 2\n"})}),"\n",(0,a.jsx)(n.p,{children:"Create a trainable LLM model and add it to the database, then the training task will run automatically."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'llm = LLM(\n    identifier="llm",\n    model_name_or_path=model_name,\n    trainer=trainer,\n    model_kwargs=model_kwargs,\n    tokenizer_kwargs=tokenizer_kwargs,\n)\n\ndb.apply(llm)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"load-the-trained-model",children:"Load the trained model"}),"\n",(0,a.jsx)(n.p,{children:"There are two methods to load a trained model:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Load the model directly"}),": This will load the model with the best metrics (if the transformers' best model save strategy is set) or the last version of the model."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use a specified checkpoint"}),": This method downloads the specified checkpoint, then initializes the base model, and finally merges the checkpoint with the base model. This approach supports custom operations such as resetting flash_attentions, model quantization, etc., during initialization."]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# <tab: Load Trained Model Directly>\nllm = db.load("model", "llm")\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# <tab: Use a specified checkpoint>\nfrom superduper_transformers import LLM\n\nexperiment_id = db.show("checkpoint")[-1]\nversion = None # None means the last checkpoint\ncheckpoint = db.load("checkpoint", experiment_id, version=version)\nllm = LLM(\n    identifier="llm",\n    model_name_or_path=model_name,\n    adapter_id=checkpoint,\n    model_kwargs=dict(load_in_4bit=True)\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"llm.predict(input_text, max_new_tokens=200)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import Template\n\nt = Template('llm-finetune', template=llm, substitutions={'docs': 'collection', model_name: 'model_name'})\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"t.export('.')\n"})})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var a=t(6540);const s={},r=a.createContext(s);function i(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);