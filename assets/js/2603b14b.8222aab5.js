"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[1041],{9971:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var a=t(4848),i=t(8453);const r={},s="Multimodal vector search - images",o={id:"templates/multimodal_image_search",title:"Multimodal vector search - images",description:"Get useful sample data",source:"@site/docs/templates/multimodal_image_search.md",sourceDirName:"templates",slug:"/templates/multimodal_image_search",permalink:"/docs/next/templates/multimodal_image_search",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/docs/templates/multimodal_image_search.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Fine tune LLM on database",permalink:"/docs/next/templates/llm_finetuning"},next:{title:"Multimodal vector search - Video",permalink:"/docs/next/templates/multimodal_video_search"}},d={},l=[{value:"Get useful sample data",id:"get-useful-sample-data",level:2},{value:"Build multimodal embedding models",id:"build-multimodal-embedding-models",level:2},{value:"Create vector-index",id:"create-vector-index",level:2},{value:"Add the data",id:"add-the-data",level:2},{value:"Perform a vector search",id:"perform-a-vector-search",level:2},{value:"Create a <code>Template</code>",id:"create-a-template",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"multimodal-vector-search---images",children:"Multimodal vector search - images"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"APPLY = False\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import superduper\n\ndb = superduper('mongomock:///test_db')\n"})}),"\n",(0,a.jsx)(n.h2,{id:"get-useful-sample-data",children:"Get useful sample data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def getter():\n    import subprocess\n    subprocess.run([\n        'curl', '-O', 'https://superduperdb-public-demo.s3.amazonaws.com/images_classification.zip',\n    ])\n    subprocess.run(['rm', '-rf', 'images'])\n    subprocess.run(['rm', '-rf', '__MACOSX'])\n    subprocess.run(['unzip', 'images_classification.zip'])\n    subprocess.run(['rm', 'images_classification.zip'])\n    import json\n    from PIL import Image\n    with open('images/images.json', 'r') as f:\n        data = json.load(f)\n    data = data[:100]\n    data = [{'img': Image.open(r['image_path'])} for r in data]\n    subprocess.run(['rm', '-rf', '__MACOSX'])\n    subprocess.run(['rm', '-rf', 'images'])\n    return data\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    data = getter()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"build-multimodal-embedding-models",children:"Build multimodal embedding models"}),"\n",(0,a.jsx)(n.p,{children:"We define the output data type of a model as a vector for vector transformation."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper.components.vector_index import sqlvector\n\noutput_datatype = sqlvector(shape=(1024,))\n"})}),"\n",(0,a.jsx)(n.p,{children:"Then define two models, one for text embedding and one for image embedding."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import clip\nfrom superduper import vector, imported\nfrom superduper_torch import TorchModel\n\nrn50 = imported(clip.load)('RN50', device='cpu')\n\ncompatible_model = TorchModel(\n    identifier='clip_text',\n    object=rn50[0],\n    preprocess=lambda x: clip.tokenize(x)[0],\n    postprocess=lambda x: x.tolist(),\n    datatype=output_datatype,\n    forward_method='encode_text',\n)\n\nembedding_model = TorchModel(\n    identifier='clip_image',\n    object=rn50[0].visual,\n    preprocess=rn50[1],\n    postprocess=lambda x: x.tolist(),\n    datatype=output_datatype,\n)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Because we use multimodal models, we define different keys to specify which model to use for embedding calculations in the vector_index."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"indexing_key = 'img'\ncompatible_key = 'text'\n"})}),"\n",(0,a.jsx)(n.h2,{id:"create-vector-index",children:"Create vector-index"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"vector_index_name = 'my-vector-index'\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import VectorIndex, Listener\n\nvector_index = VectorIndex(\n    vector_index_name,\n    indexing_listener=Listener(\n        key=indexing_key,\n        select=db['docs'].select(),\n        model=embedding_model,\n        identifier='indexing-listener',\n    ),\n    compatible_listener=Listener(\n        key=compatible_key,\n        model=compatible_model,\n        select=None,\n        identifier='compatible-listener',\n    )\n)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import Application\n\napplication = Application(\n    'image-vector-search',\n    components=[vector_index],\n)\n\nif APPLY:\n    db.apply(application, force=True)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"add-the-data",children:"Add the data"}),"\n",(0,a.jsxs)(n.p,{children:["The order in which data is added is not important. ",(0,a.jsx)(n.em,{children:"However"})," if your data requires a custom ",(0,a.jsx)(n.code,{children:"Schema"})," in order to work, it's easier to add the ",(0,a.jsx)(n.code,{children:"Application"})," first, and the data later. The advantage of this flexibility, is that once the ",(0,a.jsx)(n.code,{children:"Application"})," is installed, it's waiting for incoming data, so that the ",(0,a.jsx)(n.code,{children:"Application"})," is always up-to-date. This comes in particular handy with AI scenarios which need to respond to changing news."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    from superduper import Document\n    \n    table_or_collection = db['docs']\n    \n    ids = db.execute(table_or_collection.insert([Document(r) for r in data]))\n"})}),"\n",(0,a.jsx)(n.h2,{id:"perform-a-vector-search",children:"Perform a vector search"}),"\n",(0,a.jsx)(n.p,{children:"We can perform the vector searches using two types of data:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Text: By text description, we can find images similar to the text description."}),"\n",(0,a.jsx)(n.li,{children:"Image: By using an image, we can find images similar to the provided image."}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'if APPLY:\n    item = Document({compatible_key: "Find a black dog."})\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    from IPython.display import display\n    search_image = data[0]\n    display(search_image)\n    item = Document(search_image)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Once we have this search target, we can execute a search as follows."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"if APPLY:\n    select = db['docs'].like(item, vector_index=vector_index_name, n=5).select()\n\n    results = list(db.execute(select))\n\n    from IPython.display import display\n    for result in results:\n        display(result[indexing_key])\n"})}),"\n",(0,a.jsxs)(n.h2,{id:"create-a-template",children:["Create a ",(0,a.jsx)(n.code,{children:"Template"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from superduper import Template, Table, Schema\nfrom superduper.components.dataset import RemoteData\nfrom superduper_pillow import pil_image\n\n\ntemplate = Template(\n    'multimodal_image_search',\n    template=application,\n    default_table=Table(\n        'sample_multimodal_image_search', \n        schema=Schema(\n            'sample_multimodal_image_search/schema',\n            fields={'img': pil_image},\n        ),\n        data=RemoteData('sample_images', getter=getter),\n    ),\n    substitutions={'docs': 'table_name', 'cpu': 'device'},\n    types={\n        'device': {\n            'type': 'str',\n            'default': 'cpu',\n        },\n        'table_name': {\n            'type': 'str',\n            'default': 'sample_multimodal_image_search',\n        },\n    }\n)\n\ntemplate.export('.')\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"template.template\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"vector_index.indexing_listener.select\n"})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var a=t(6540);const i={},r=a.createContext(i);function s(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);