"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[7108],{7440:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>l,toc:()=>d});var s=r(4848),i=r(8453);const t={},a="Transformers",l={id:"ai_plugins/transformers",title:"Transformers",description:"Installation",source:"@site/content/ai_plugins/transformers.md",sourceDirName:"ai_plugins",slug:"/ai_plugins/transformers",permalink:"/docs/ai_plugins/transformers",draft:!1,unlisted:!1,editUrl:"https://github.com/superduper-io/superduper/edit/main/docs/content/ai_plugins/transformers.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Scikit-learn",permalink:"/docs/ai_plugins/sklearn"},next:{title:"vLLM",permalink:"/docs/ai_plugins/vllm"}},o={},d=[{value:"Installation",id:"installation",level:2},{value:"API",id:"api",level:2},{value:"<code>TextClassification</code>",id:"textclassification",level:3},{value:"<code>LLM</code>",id:"llm",level:3},{value:"Training",id:"training",level:2},{value:"LLM fine-tuning",id:"llm-fine-tuning",level:3},{value:"Supported Features",id:"supported-features",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"transformers",children:"Transformers"}),"\n",(0,s.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install superduper_transformers\n"})}),"\n",(0,s.jsx)(n.h2,{id:"api",children:"API"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/index",children:"Transformers"})," is a popular AI framework, and we have incorporated native support for Transformers to provide essential Large Language Model (LLM) capabilities.\n",(0,s.jsx)(n.code,{children:"superduper"})," allows users to work with arbitrary ",(0,s.jsx)(n.code,{children:"transformers"})," pipelines, with custom input/ output data-types."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Class"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"GitHub"}),(0,s.jsx)(n.th,{children:"API-docs"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"superduper.ext.transformers.model.TextClassification"})}),(0,s.jsx)(n.td,{children:"A pipeline for classifying text."}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://github.com/superduper/superduper/blob/main/superduper/transformers/model.py",children:"Code"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"/docs/api/ext/transformers/model#textclassificationpipeline",children:"Docs"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"superduper.ext.transformers.model.LLM"})}),(0,s.jsxs)(n.td,{children:["Work locally with the ",(0,s.jsx)(n.code,{children:"transformers"})," implementations of LLM."]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://github.com/superduper/superduper/blob/main/superduper/ext/transformers/model.py",children:"Code"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"/docs/api/ext/transformers/model#llm",children:"Docs"})})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"textclassification",children:(0,s.jsx)(n.code,{children:"TextClassification"})}),"\n",(0,s.jsxs)(n.p,{children:["One of the most commonly used pipelines in ",(0,s.jsx)(n.code,{children:"transformers"})," is the ",(0,s.jsx)(n.code,{children:"TextClassificationPipeline"}),".\nYou may apply and train these pipelines with ",(0,s.jsx)(n.code,{children:"superduper"}),".\nRead more in the ",(0,s.jsx)(n.a,{href:"/docs/api/ext/transformers/model#textclassificationpipeline",children:"API documentation"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"llm",children:(0,s.jsx)(n.code,{children:"LLM"})}),"\n",(0,s.jsx)(n.p,{children:"You can quickly utilize LLM capabilities using the following Python function:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from superduper.ext.transformers import LLM\nllm = LLM(model_name_or_path="facebook/opt-350m")\nllm.predict("What are we having for dinner?")\n'})}),"\n",(0,s.jsx)(n.p,{children:"Or use a method similar to transformers\u2019 from_pretrained, just need to supplement the identifier parameter."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from superduper.ext.transformers import LLM\nllm = LLM.from_pretrained(\n    "facebook/opt-350m", \n    load_in_8bit=True, \n    device_map="cuda", \n    identifier="llm",\n)\n'})}),"\n",(0,s.jsx)(n.p,{children:"The model can be configured with the following parameters:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"adapter_id: Add an adapter to the base model for inference."}),"\n",(0,s.jsx)(n.li,{children:"model_kwargs: a dictionary; all the model_kwargs will be passed to transformers.AutoModelForCausalLM.from_pretrained. You can provide parameters such as trust_remote_code=True."}),"\n",(0,s.jsx)(n.li,{children:"tokenizer_kwargs: a dictionary; all the tokenizer_kwargs will be passed to transformers.AutoTokenizer.from_pretrained."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"training",children:"Training"}),"\n",(0,s.jsxs)(n.p,{children:["For a fully worked out training/ fine-tuning use-case refer to the ",(0,s.jsx)(n.a,{href:"../use_cases/fine_tune_llm_on_database.md",children:"use-cases section"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"llm-fine-tuning",children:"LLM fine-tuning"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"superduper"})," provides a convenient fine-tuning method based on the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/trl/index",children:"trl"})," framework to help you train data in the database."]}),"\n",(0,s.jsx)(n.h3,{id:"supported-features",children:"Supported Features"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Training Methods"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Full fine-tuning"}),"\n",(0,s.jsx)(n.li,{children:"LoRA fine-tuning"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parallel Training"}),":"]}),"\n",(0,s.jsxs)(n.p,{children:["Parallel training is supported using Ray, with data parallelism as the default strategy. You can also pass DeepSpeed parameters to configure parallelism through ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/main_classes/deepspeed#zero",children:"DeepSpeed configuration"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Multi-GPUs fine-tuning"}),"\n",(0,s.jsx)(n.li,{children:"Multi-nodes fine-tuning"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Training on Ray"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"We can use Ray to train models. When using Ray as the compute backend, tasks will automatically run in Ray and the program will no longer be blocked."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>l});var s=r(6540);const i={},t=s.createContext(i);function a(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);