{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b24af19",
   "metadata": {},
   "source": [
    "# Training and Managing MNIST Predictions with superduper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8905783f",
   "metadata": {},
   "source": [
    ":::note\n",
    "This tutorial guides you through the implementation of a classic machine learning task: MNIST handwritten digit recognition. The twist? We perform the task directly on data hosted in a database using superduper.\n",
    ":::\n",
    "\n",
    "This example makes it easy to connect any of your image recognition models directly to your database in real-time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbf3ba-a89f-4a23-b941-ce4dab500ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3812091",
   "metadata": {},
   "source": [
    "First, we need to establish a connection to a MongoDB datastore via superduper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28adbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import superduper\n",
    "    \n",
    "db = superduper('mongomock://')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233e891",
   "metadata": {},
   "source": [
    "After establishing a connection to MongoDB, the next step is to load the MNIST dataset. superduper's strength lies in handling diverse data types, especially those that are not supported by standard databases. To achieve this, we use an `Encoder` in conjunction with `Document` wrappers. These components allow Python dictionaries containing non-JSONable or bytes objects to be seamlessly inserted into the underlying data infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0934cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from superduper import Document\n",
    "\n",
    "import random\n",
    "\n",
    "# Load MNIST images as Python objects using the Python Imaging Library.\n",
    "# Each MNIST item is a tuple (image, label)\n",
    "mnist_data = list(torchvision.datasets.MNIST(root='./data', download=True))\n",
    "\n",
    "document_list = [Document({'img': x[0], 'class': x[1]}) for x in mnist_data]\n",
    "\n",
    "# Shuffle the data and select a subset of 1000 documents\n",
    "random.shuffle(document_list)\n",
    "data = document_list[:1000]\n",
    "\n",
    "# Insert the selected data into the mnist_collection which we mentioned before like: mnist_collection = Collection('mnist')\n",
    "db['mnist'].insert_many(data[:-100]).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5341135",
   "metadata": {},
   "source": [
    "Now that the images and their classes are inserted into the database, we can query the data in its original format. Particularly, we can use the `PIL.Image` instances to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display one of the images\n",
    "r = db['mnist'].find_one().execute()\n",
    "r.unpack()['img'].resize((300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fde8bb",
   "metadata": {},
   "source": [
    "Following that, we build our machine learning model. superduper conveniently supports various frameworks, and for this example, we opt for PyTorch, a suitable choice for computer vision tasks. In this instance, we combine `torch` with `torchvision`.\n",
    "\n",
    "To facilitate communication with the superduper `Datalayer`, we design `postprocess` and `preprocess` functions. These functions are then wrapped with the `TorchModel` wrapper to create a native superduper object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb425e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper.ext.torch import TorchModel\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define the LeNet-5 architecture for image classification\n",
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Layer 1\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            torch.nn.BatchNorm2d(6),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # Layer 2\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # Fully connected layers\n",
    "        self.fc = torch.nn.Linear(400, 120)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc1 = torch.nn.Linear(120, 84)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Postprocess function for the model output    \n",
    "def postprocess(x):\n",
    "    return int(x.topk(1)[1].item())\n",
    "\n",
    "# Preprocess function for input data\n",
    "def preprocess(x):\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((32, 32)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=(0.1307,), std=(0.3081,))]\n",
    "    )(x)\n",
    "\n",
    "# Create an instance of the LeNet-5 model\n",
    "lenet_model = LeNet5(10)\n",
    "\n",
    "\n",
    "model = TorchModel(\n",
    "    identifier='my-model',\n",
    "    object=lenet_model,\n",
    "    preprocess=preprocess,\n",
    "    postprocess=postprocess, \n",
    "    preferred_devices=('cpu',),\n",
    ")\n",
    "\n",
    "# Check that the model successfully creates predictions over single data-points\n",
    "model.predict(data[0]['img'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf0457e",
   "metadata": {},
   "source": [
    "Now we are ready to \"train\" or \"fit\" the model. Trainable models in superduper come with a sklearn-like `.fit` method,\n",
    "which developers may implement for their specific model class. `torch` models come with a pre-configured\n",
    "`TorchTrainer` class and `.fit` method. These may be invoked simply by \"applying\" the model to `db`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c610c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from superduper import Metric, Validation, Dataset\n",
    "from superduper.ext.torch import TorchTrainer\n",
    "\n",
    "acc = lambda x, y: (sum([xx == yy for xx, yy in zip(x, y)]) / len(x))\n",
    "\n",
    "accuracy = Metric(identifier='acc', object=acc)\n",
    "\n",
    "model.validation = Validation(\n",
    "    'mnist_performance',\n",
    "    datasets=[\n",
    "        Dataset(\n",
    "            identifier='my-valid',\n",
    "            select=db['mnist'].find({'_fold': 'valid'})\n",
    "        )\n",
    "    ],\n",
    "    metrics=[accuracy],\n",
    ")\n",
    "\n",
    "model.trainer = TorchTrainer(\n",
    "    identifier='my-trainer',\n",
    "    objective=cross_entropy,\n",
    "    loader_kwargs={'batch_size': 10},\n",
    "    max_iterations=1000,\n",
    "    validation_interval=5,\n",
    "    select=db['mnist'].find(),\n",
    "    key=('img', 'class'),\n",
    "    transform=lambda x, y: (preprocess(x), y),\n",
    ")\n",
    "\n",
    "_ = db.apply(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b95b68-ea86-47d5-8eef-8677144339ff",
   "metadata": {},
   "source": [
    "The trained model is now available via `db.load` - the `model.trainer` object contains the metric traces\n",
    "logged during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the model from the database\n",
    "model = db.load('model', model.identifier)\n",
    "\n",
    "# Plot the accuracy values\n",
    "plt.plot(model.trainer.metric_values['my-valid/acc'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
